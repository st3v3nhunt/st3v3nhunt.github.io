[{"content":"Ever wanted to know what value a variable has and where it was set from in Vim?\nEasy. Just run:\n:verbose set \u0026lt;option\u0026gt; e.g. for textwidth\n:verbose set textwidth The output would look something like:\ntextwidth=120 Last set from ~/code/dotfiles/vim/ftplugin/markdown.vim line 4 ","permalink":"https://blog.st3v3nhunt.me/til/vim/what-set-a-value/","summary":"\u003cp\u003eEver wanted to know what value a variable has and where it was set from in Vim?\u003c/p\u003e","title":"üìì What set this value?"},{"content":"Whenever I have used the GOV.UK Prototype Kit to build a prototype I have always hosted it on Heroku. The main reason for doing this was because it was just so easy (I wrote a post about just how easy it was many years ago), it was also the default approach - until recently.\nHeroku\u0026rsquo;s next chapter At the end of August 2022 Heroku communicated they were going to be\nphasing out our free plan for Heroku Dynos\nUnfortunately this was the plan used for all of the prototypes I had setup and I would imagine almost all of the other GOV.UK prototypes running on the platform too (although I have no data to back this up).\nThe best thing about it being free was that you could get it setup with a personal account, get working on it quickly and then not worry about how long it might take to go through the discussions internally about who and how it would be managed and maintained. Because it wasn\u0026rsquo;t going to cost you anything it didn\u0026rsquo;t matter if those discussions took some time or indeed, if they never happened at all (which isn\u0026rsquo;t necessarily the best idea but in reality a prototype should be perfectly OK being managed by the team doing the work with the prototype).\nSince the initial announcement, Heroku have said they will introduce a new low-cost plan called Eco, providing 1000 hours across all Eco dynos for $5 a month. So if you wanted to continue to use Heroku it isn\u0026rsquo;t going to cost much, although you might need to go through the red tape and discussions to get it paid for!\nIn all seriousness, if you are already using Heroku, the easiest and most effective option should be to just go for a paid for plan.\nAlternative hosting providers With the plan changes and the nuisance a security incident in April 2022 when GitHub Integration with Heroku was disabled caused, I had been considering trying alternatives to Heroku.\nPlus, this sort of stuff is interesting and good to look at every so often! So, if you wanted to use another hosting provider (why not try something different, you might like it üòÉ) there are several to choose from.\nGOV.UK suggests:\nRailway Render I\u0026rsquo;ve tried both to see what they were like, below is a short summary of what I found. Note that this is not a detailed comparison and doesn\u0026rsquo;t consider the paid offering, I really just wanted to see what you could get for free.\nRailway Railway is very easy to get started with due to the GitHub app integration.\nDuring the deployment flow you are asked if there are any environment variables you want to setup. For a GOV.UK prototype there are two env vars that should be set:\nNODE_ENV=production - sets the runtime environment to production which will result in a password being required to access the site PASSWORD=\u0026lt;whatever_you_want\u0026gt; - sets the password to access the site A note on the password - it is important to set NODE_ENV=production. Without this the application will not prompt for a password. And you really want a password prompt to protect people and prevent them from stumbling across the prototype and potentially mistaking it for a real service.\nAfter running through the deploy flow and setting the env vars the site was running as expected - very straightforward.\nA draw back of Railway is, on the Starter Plan the service is limited to running for 500 hours which isn\u0026rsquo;t enough to run all day every day. This wouldn\u0026rsquo;t be a problem if the service could easily be stopped and started when needed, however (at the time of writing) I can\u0026rsquo;t see a way to do this beyond removing the service and then adding it back.\nThe process is reasonably straight forward and if it could be automated it wouldn\u0026rsquo;t really be an issue but the CLI doesn\u0026rsquo;t appear to provide the ability to do this. It ends up being a bit of a nuisance as the env vars (and other data) are deleted each time resulting in a manual process taking ~5 mins of time (to delete and recreate) needs to be done every weekend in order to just about be within the limits. 5 minutes isn\u0026rsquo;t much to \u0026lsquo;pay\u0026rsquo; for the use of the service but if it isn\u0026rsquo;t done then you might find you have run out hours for the month!\nOverall I would consider using Railway for future prototype hosting due to its ease of use and now I understand the limitations.\nRender Render is very easy to get started with due to the GitHub app integration.\nWhen setting the service up, both the build and start commands will need to be entered (npm install \u0026amp;\u0026amp; npm run build and npm run start respectively, at the time of writing). The interface also provides the ability to enter the env vars. Differing from Railway, Render only requires the PASSWORD to be set as NODE_ENV is set to production by default - a nice touch as this also means the site doesn\u0026rsquo;t work rather than being accessible on first deployment.\nOnce the service was configured it seemed to take a bit longer to startup and get running than the Railway service.\nI think the main benefit of Render over Railway is the 750 hours the Free Plan provides (it can be on all of the time). There are some limitations but they shouldn\u0026rsquo;t be a problem for the typical use cases a prototype fulfills i.e. demos and user research. The main thing to be aware of is the service being \u0026lsquo;spun down\u0026rsquo; after 15 minutes of activity. If that was a major concern I\u0026rsquo;m sure setting up an uptime status checker such as UptimeRobot or statuscake would prevent the site from being spun down. If neither of those works using a Function as a service (FaaS) provider such as AWS Lambda or Azure Functions with their healthy free quotas could be used to request the site on a regular schedule!\nConclusion The next prototype I need to host I would choose Render over Railway, mainly because of the 750 hours. However, it also feels more mature in terms of documentation, configurability and services offered.\n","permalink":"https://blog.st3v3nhunt.me/posts/hosting-govuk-prototypes/","summary":"\u003cp\u003eWhenever I have used the\n\u003ca href=\"https://govuk-prototype-kit.herokuapp.com/\"\u003eGOV.UK Prototype Kit\u003c/a\u003e\nto build a prototype I have always hosted it on\n\u003ca href=\"https://www.heroku.com/\"\u003eHeroku\u003c/a\u003e. The main reason for doing this was because\nit was just so easy (I wrote a\n\u003ca href=\"./posts/how-to-setup-a-simple-heroku-pipeline/\"\u003epost\u003c/a\u003e about just how easy it\nwas many years ago), it was also the default approach - until recently.\u003c/p\u003e","title":"‚òÅÔ∏è  Hosting GOV.UK prototypes"},{"content":"Accessing an environment variable within Neovim from a lua script is straight forward. It is just a case of calling vim.env.env-var-name where env-var-name is the name of the environment variable e.g.\nlocal homebrew_prefix = vim.env.HOMEBREW_PREFIX print(homebrew_prefix) Would print the value of the $HOMEBREW_PREFIX env var.\n","permalink":"https://blog.st3v3nhunt.me/til/vim/access-env-vars-neovim/","summary":"\u003cp\u003eAccessing an environment variable within Neovim from a lua script is straight\nforward. It is just a case of calling \u003ccode\u003evim.env.env-var-name\u003c/code\u003e where\n\u003ccode\u003eenv-var-name\u003c/code\u003e is the name of the environment variable e.g.\u003c/p\u003e","title":"üìì Access env vars in Neovim"},{"content":"Within GitHub, if you belong to multiple organisations it is easy to have those notifications routed to specific email addresses. This is very useful if, for example, you use the same GitHub account for work and personal use.\nThe first thing to do is to ensure you have added the email address to your account. Then follow these instructions to customise the routing.\n","permalink":"https://blog.st3v3nhunt.me/til/git/custom-email-routing-per-organization-in-github/","summary":"\u003cp\u003eWithin \u003ca href=\"https://github.com\"\u003eGitHub\u003c/a\u003e, if you belong to multiple organisations it\nis easy to have those notifications routed to specific email addresses. This is\nvery useful if, for example, you use the same GitHub account for work and\npersonal use.\u003c/p\u003e","title":"üêô Custom email routing per organization in GitHub"},{"content":"Within a Svelte project seeing an error such as cannot find module '$lib/..' or its corresponding type declarations can be resolved by running npm run prepare. This will update tsconfig.json to add $lib to compilerOptions.paths.\n","permalink":"https://blog.st3v3nhunt.me/til/sveltejs/fix-missing-module-error/","summary":"\u003cp\u003eWithin a Svelte project seeing an error such as\n\u003ccode\u003ecannot find module '$lib/..' or its corresponding type declarations\u003c/code\u003e can be\nresolved by running \u003ccode\u003enpm run prepare\u003c/code\u003e. This will update \u003ccode\u003etsconfig.json\u003c/code\u003e to add\n\u003ccode\u003e$lib\u003c/code\u003e to\n\u003ca href=\"https://www.typescriptlang.org/tsconfig#paths\"\u003ecompilerOptions.paths\u003c/a\u003e.\u003c/p\u003e","title":"üî® Fix missing module error"},{"content":"Decisions are made all of the time when developing software. Some decisions are small and easy to change such as the name of a variable or whether to extract some code into a separate class/function/module. Other decisions are larger and more difficult to change such as which test framework to use, whether to use JSON or XML or if a NoSQL database should be used in favour of a SQL database, etc.\nMaking decisions When a decision is made, the person making it should understand a number of factors about the decision including the context within which it is being made, the reason(s) for making it and what the potential impact or consequence(s) of it will be. It is generally straightforward to communicate the reason(s) and consequence(s) to other people, however, the context is less obvious as it is often taken for granted that other people implicitly understand it. Often this is not the case.\nThis is especially true if the team is large and/or working in a fluid environment. There are other situations where context, and indeed the other information pertaining to the decision is not available to learn from. This can include when new people join the team or indeed your future self.\nADRs This is where ADRs come into play. An ADR or Architectural Decision Record is a way of recording important decisions you want to record for others (including your future self) so they can learn and help to understand why something was done the way it was.\nIt is easy to review a software system and be critical of how it has been implemented and provide suggestions about how this or that could be done better. Doing this without the context as to why a particular approach has been taken is much less valuable.\nIt is much more valuable to make suggestions that haven\u0026rsquo;t been tried before rather than suggesting the same thing again. Or recommending an approach with an appreciation of how a similar approach didn\u0026rsquo;t work previously.\nIf the person(s) responsible for making the decision(s) in the past is available to explain why then some of this information will be able to be communicated, however, people\u0026rsquo;s memories are fallible. Having a written record is vastly superior. This is a situation where ADRs come into their own.\nAnother benefit or writing an ADR is to help the decision maker(s) more thoroughly understand the rationale for the decision, the options available at the time and the consequence(s) of making it. It really is very helpful to capture the other options that were available including the reason(s) why these were not chosen.\nMADRs A more recent and more generalised version of an ADR exists, a MADR - Markdown Any Decision Records.\nMADRs appear to have various forms (based on how much information is required to capture the decision). There are examples for a short version and a long version.\nSome of the changes look really positive such as being explicit about the Considered Options along with the consequences being split into Positive Consequences and Negative Consequences. I can imagine this helps the author to structure their thought process and ensure the decision is both considered and information is not omitted from the written record.\nI\u0026rsquo;ve not used MADRs but would consider them in the future.\nCreating and managing ADRs When to create an ADR If you are using ADRs a couple of questions will inevitably crop up.\nThe first will be When should I create an ADR?. Inevitably, the answer is it depends. The guidance I would give is, anything:\nyour future self would ask why it was done this way you have been questioned about and had to explain that has been discussed and debated where there were several options to choose from that deviates from existing standards Some examples:\nUsing a 15 minute timeout for a magic link Using StandardJS rather than ESLint Making the code private rather than public Using C# rather than JavaScript Using Svelte rather than React How to manage ADRs The second question is likely to be related to how ADRs should be managed. This is especially pertinent when the decision impacts multiple code repositories, a likely scenario when using microservices.\nAs above, the answer isn\u0026rsquo;t necessarily straightforward as it depends on the context of the team/project/organisation in which you are working.\nADRs should be co-located with the code where the decision has an impact. This should always be the case. If this isn\u0026rsquo;t happening, a lot of the value provided by ADRs is lost. People working on the code might not even know ADRs are being used in which case any new decisions are unlikely to be recorded, and learnings from previous decisions are not available to provide context to the team.\nIn situations where an ADR spans multiple repositories this might suggest the decision is dealing with a standard. Ideally there are mechanisms for recording standards outside of the repository, this might be at the project or organisation level.\nIf the ADR is not a standard and thus will not be recorded elsewhere and is applicable to multiple repositories I would suggest an ADR is added to each impacted repository, ideally with a link to the related ADR(s) to aid in the documentation.\nWhy use an ADR rather than commit messages? A good commit message should contain the reason for the change rather than simply recording what the change is. Therefore, you might reasonably ask why commit messages shouldn\u0026rsquo;t be used to record the information being added to the ADR?\nThere are a number of reasons:\nCommit messages are much less visible than ADRs Commit messages are generally for small chunks of work and it is likely several commits would span what is covered by a single ADR Tooling exists to create websites out of ADRs, further increasing visibility and ease of use ","permalink":"https://blog.st3v3nhunt.me/posts/using-adrs/","summary":"\u003cp\u003eDecisions are made all of the time when developing software. Some decisions are\nsmall and easy to change such as the name of a variable or whether to extract\nsome code into a separate class/function/module. Other decisions are larger and\nmore difficult to change such as which test framework to use, whether to use\nJSON or XML or if a NoSQL database should be used in favour of a SQL database,\netc.\u003c/p\u003e","title":"üìö Using Architectural Decision Records"},{"content":"It is possible to build, create, start and up a specific service or services from Docker Compose. By default these commands work with an optional list of services. Each service specified will have the command run for it.\nThis is handy should only some of the services want to be started and can come in handy for testing or just bringing up the DB without all other services.\nAs an alternative, Compose has the ability to use profiles which can make it easier to manage selective service actions in larger docker-compose.yml files.\n","permalink":"https://blog.st3v3nhunt.me/til/docker/build-create-start-named-services-from-docker-compose/","summary":"\u003cp\u003eIt is possible to\n\u003ca href=\"https://docs.docker.com/engine/reference/commandline/compose_build/\"\u003ebuild\u003c/a\u003e,\n\u003ca href=\"https://docs.docker.com/engine/reference/commandline/compose_create/\"\u003ecreate\u003c/a\u003e,\n\u003ca href=\"https://docs.docker.com/engine/reference/commandline/compose_start/\"\u003estart\u003c/a\u003e\nand \u003ca href=\"https://docs.docker.com/engine/reference/commandline/compose_uo/\"\u003eup\u003c/a\u003e\na specific service or services from Docker Compose. By default these commands\nwork with an optional list of services. Each service specified will have the\ncommand run for it.\u003c/p\u003e","title":"ü™Ö Build, create and start named services from Docker Compose"},{"content":"From version 3.8 of the compose file specification the version key is no longer required. However, it can be kept for information, if so desired.\nIn order to run 3.8+ Docker Engine 19.03.0+ is required, see the compatibility matrix for details.\nThe latest Compose file specification is a merging of the 2.x and 3.x versions and should be the preferred option.\n","permalink":"https://blog.st3v3nhunt.me/til/docker/docker-compose-version-no-longer-required/","summary":"\u003cp\u003eFrom version\n\u003ca href=\"https://docs.docker.com/compose/compose-file/compose-versioning/#version-38\"\u003e3.8\u003c/a\u003e\nof the compose file specification the version key is no longer required.\nHowever, it can be kept for\n\u003ca href=\"https://docs.docker.com/compose/compose-file/#version-top-level-element\"\u003einformation\u003c/a\u003e,\nif so desired.\u003c/p\u003e","title":"üìì Docker Compose version key is not required after 3.7"},{"content":"It is straight forward to order content in Hugo by leveraging front matter variables. The default order is Weight -\u0026gt; Date -\u0026gt; LinkTitle -\u0026gt; FilePath. It is easy to change the order based on the specific property of interest. See lists of content in Hugo for more information.\nThe posts in TIL are ordered by weight. This involved added weight (an integer) to the front matter of the _index.md files in the folders within the til folder.\nRather than weight each letter with the place it has in the alphabet the number has been increased by a factor of 10. This means it will be easy to add another 9 words beginning with the same letter. If it gets to the point of needing more than 10 words with the same letter the weighting will need to be rejigged, however, I do not anticipate this being the case.\n","permalink":"https://blog.st3v3nhunt.me/til/hugo/ordering-content/","summary":"\u003cp\u003eIt is straight forward to order content in Hugo by leveraging front matter\nvariables. The default order is \u003ccode\u003eWeight -\u0026gt; Date -\u0026gt; LinkTitle -\u0026gt; FilePath\u003c/code\u003e.\nIt is easy to change the order based on the specific property of interest. See\n\u003ca href=\"https://gohugo.io/templates/lists/#order-content\"\u003elists of content in Hugo\u003c/a\u003e\nfor more information.\u003c/p\u003e","title":"‚ÜïÔ∏è  Ordering content"},{"content":"Add a misspelt word to Vim\u0026rsquo;s dictionary with zg.\nSee the list of suggestions for a misspelt word with z=. Enter the number of the correct suggestion and hit enter to select it.\nIf you pre-emptively know the number of the suggestion you can choose that by entering \u0026lt;number\u0026gt;z=. Although knowing what number to choose is a little tricky, perhaps the first suggestion is worth a punt?\n","permalink":"https://blog.st3v3nhunt.me/til/vim/interacting-with-vims-dictionary/","summary":"\u003cp\u003eAdd a misspelt word to Vim\u0026rsquo;s dictionary with \u003ccode\u003ezg\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSee the list of suggestions for a misspelt word with \u003ccode\u003ez=\u003c/code\u003e. Enter the number of\nthe correct suggestion and hit enter to select it.\u003c/p\u003e","title":"üìì Interacting with vim's dictionary"},{"content":"Over the years I\u0026rsquo;ve tried to keep a record of what I\u0026rsquo;ve done in some form or other. I\u0026rsquo;ve experimented with week notes (effectively a summary of what you did over the past week that you might make public, or just keep private).\nI\u0026rsquo;ve also made notes focussed on a sprint so I am better prepared for the retro. I\u0026rsquo;ve also tried daily notes, partly so I could use them for the update at stand-ups.\nUnfortunately, none of these have lasted more than a few months before I break the habit. There isn\u0026rsquo;t a single reason this happens, more so that there are number of contributing factors:\nWhether it was weekly or daily, I was attempting to stick to a set schedule and when I eventually broke the streak I found it difficult to reach the same level of consistency and then just ran out steam When I write I tend to edit constantly which results in more time being taken than I would ideally want to spend on the activity I would consider only a small subset of what I\u0026rsquo;d done as worth recording which would become a little disheartening when I would see just how much other people had recorded! Beyond the act of recording what I\u0026rsquo;d done which would provide me with the info I could use in stand-ups or retros I wasn\u0026rsquo;t entirely sure what value I was getting out of the process On reflection, I think these issues can be summarised into the four areas I\u0026rsquo;ve expanded on below.\nFrequency To reduce the issue about sticking to a schedule I\u0026rsquo;m simply going to attempt to capture something daily or weekly or perhaps just monthly and not worry if that doesn\u0026rsquo;t happen. Ideally I\u0026rsquo;ll record something daily more often than not and have a weekly record almost all of them time but I think the key is not being tied to a schedule.\nTime taken I\u0026rsquo;m going to try and edit less whilst I\u0026rsquo;m doing the writing, however, I think that is more easily said than done! I\u0026rsquo;ll probably read a few blogs about how to improve and potentially look if there are any writing courses that could help me improve my writing speed.\nRecord everything Maybe not everything! However, I\u0026rsquo;ll certainly be recording more stuff than I would have previously. I\u0026rsquo;m going to record much smaller achievements, learnings, teachings, etc. Related to this, I\u0026rsquo;ve also started to record things I\u0026rsquo;ve learnt which I hope will encourage me to publish more frequently.\nValue I\u0026rsquo;ve read a couple of posts about brag documents which have helped me to understand the value of such a document. The value includes having a written record I can use:\nto reflect on what I have done, what I haven\u0026rsquo;t done, what I should be doing, etc. to help me understand if I am going in the direction I need to in order to achieve my goals and objectives as evidence to advocate for myself or provide to others to advocate on my behalf during reviews to help with interview preparation by providing a ready generated list of examples to use rather than having to spend time trying (and failing) to remember everything that would be good to have at the forefront of your mind Summary Whilst I\u0026rsquo;ve not detailed what a brag document contains (the posts linked above do a good job) I\u0026rsquo;ve hopefully set out some good reasons as to why it is worth having one along with some approaches on how to make it into a habit.\nWhether you call it a brag document or week notes or a work record or a dairy, it doesn\u0026rsquo;t matter, having a record of stuff you have done is a good idea. I really wish I had done this more consistently over the course of my career.\nAs a slight aside, I do wonder about those times when I felt I didn\u0026rsquo;t have much to record and whether that was an indicator it was time to look for a new challenge.\n","permalink":"https://blog.st3v3nhunt.me/posts/keep-a-brag-doc/","summary":"\u003cp\u003eOver the years I\u0026rsquo;ve tried to keep a record of what I\u0026rsquo;ve done in some form or\nother. I\u0026rsquo;ve experimented with \u003ca href=\"https://weeknot.es/\"\u003eweek notes\u003c/a\u003e (effectively a\nsummary of what you did over the past week that you might make public, or just\nkeep private).\u003c/p\u003e","title":"üì¢ Keep a brag document"},{"content":"In a Git repo run git log -1 to get details of the last commit.\n--pretty can be used to format the message according to your needs.\nI sometimes just want the subject of the last commit, in which case I run:\ngit log -1 --pretty=%s ","permalink":"https://blog.st3v3nhunt.me/til/git/get-last-commit-message/","summary":"\u003cp\u003eIn a Git repo run \u003ccode\u003egit log -1\u003c/code\u003e to get details of the last commit.\u003c/p\u003e","title":"üìì Get last Git commit message"},{"content":"A subset of Bash builtins I occasionally forget about and want a quick reference to.\nCommand Description Example unalias Remove alias unalias \u0026lt;alias\u0026gt; Source\n","permalink":"https://blog.st3v3nhunt.me/til/shell/useful-bash-builtins/","summary":"\u003cp\u003eA subset of Bash builtins I occasionally forget about and want a quick\nreference to.\u003c/p\u003e","title":"üêö Useful bash builtins"},{"content":"ripgrep is an excellent alternative to grep. One of its benefits is how it filters the files it searches.\nHowever, on occasion those normally ignored files need to be searched. I find this is especially the case when searching through my dotfiles repo. Adding the flag --hidden or -. does the job e.g.\nrg -. \u0026lt;search_term\u0026gt; ","permalink":"https://blog.st3v3nhunt.me/til/ripgrep/search-hidden-files-with-ripgrep/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/BurntSushi/ripgrep\"\u003eripgrep\u003c/a\u003e is an excellent alternative to\n\u003ca href=\"https://en.wikipedia.org/wiki/Grep\"\u003egrep\u003c/a\u003e. One of its benefits is how it\n\u003ca href=\"https://github.com/BurntSushi/ripgrep/blob/master/GUIDE.md#automatic-filtering\"\u003efilters\u003c/a\u003e\nthe files it searches.\u003c/p\u003e","title":"üîç Search hidden files with ripgrep"},{"content":"If you ever need to load a bunch of env vars into an environment it is easily accomplished by having them listed in a .env file e.g.\n# .env API_KEY=secret export USERNAME=squirrel If all of the env vars are preceded with export the file can simply be sourced with source .env. However, if any of the env vars are not preceded with export there is an option that can be set prior to sourcing the file to achieve the same result. Running\nset -a; source .env; set +a will load all env vars listed and then turn off the option of exporting all defined variables.\nSource\n","permalink":"https://blog.st3v3nhunt.me/til/shell/source-and-export-all-vars-from-a-file/","summary":"\u003cp\u003eIf you ever need to load a bunch of env vars into an environment it is easily\naccomplished by having them listed in a \u003ccode\u003e.env\u003c/code\u003e file e.g.\u003c/p\u003e","title":"üêö Source and export all vars from a file"},{"content":"Teams often have a daily catch-up, often referred to as a stand-up, where an update is provided by each member of the team. The standard approach to the daily updates is a 15 minute meeting where participants answer three questions:\nWhat did you do yesterday? What are you going to do today? Are there any blockers? In my experience it is not uncommon for 15 minutes to be exceeded. Sometimes this is for good reasons and isn\u0026rsquo;t really a problem if participants leave if they are not required for the extended discussion. However, if the meeting overruns regularly and is struggling to achieve what it sets out to do there are changes that can be made to help.\nBelow are some ideas on how to attempt to keep the meeting on time and on target.\nIdeas Focus on the work Each participant should focus on activities related to the work specific to the team. There is no need to talk about non-team activities. e.g. if you have been to a community of practice meeting or to an all staff meeting.\nRemember, it is not:\na competition to talk for the longest or list the most items a way to show how busy you are It is about:\nhighlighting impediments to your work and seeking help providing an update to the team about work relevant to the team attempting daily alignment Use written updates and do it asynchronously If there are a large number of people on the team you could try to send written updates via a messaging system such as Slack, Teams, etc. Using a couple of sentences to answer the questions is much better from the POV of allowing people to opt-in to which person\u0026rsquo;s update they consume.\nProviding updates verbally forces it into a serial procedure where everybody waits until everybody has updated. In addition to extending the duration it also opens up questions about what order the updates go in. Is there a person prompting everybody in turn or does each person prompt another (potentially leading to additional delay when people are surprised to be going next and suddenly need to unmute themselves).\nContent In general Include the id of the ticket, if updates are written as opposed to spoken What was done yesterday Ideally focus on outcomes or potentially outputs. Rather than worked on ticket #123, be more specific e.g. completed design of login screen, finished setting up deployment pipeline to pre-production, agreed and documented eligibility criteria for applications Do not list the meetings you have been to. Instead, include the outcome of the meeting e.g. do not include ways of working meeting, include made progress on ways or working. Ideally it would be more specific than that e.g. agreed entry and exit criteria for tickets moving into the backlog What will be done today Try to state something that is tangible and can be completed e.g. rather than design workshop for backoffice prefer agree initial design of backoffice with stakeholders, rather than working on ticket #123 prefer implement updated branding on login screen Consider listing meetings you are going to, particularly if they are relevant to other team members who might not have been invited. However, bare in mind this is an easy way to have meeting attendance balloon. ","permalink":"https://blog.st3v3nhunt.me/posts/daily-updates/","summary":"\u003cp\u003eTeams often have a daily catch-up, often referred to as a stand-up, where an\nupdate is provided by each member of the team. The standard approach to the\ndaily updates is a 15 minute meeting where participants answer three questions:\u003c/p\u003e","title":"üóìÔ∏è  Daily updates"},{"content":"When asking for a review of a pull request (PR) ensure there is suitable context provided to the reviewer(s).\nProbably the easiest way to do this is to add information to the description of the PR. This way, anybody can see it and the information is retained for future users.\nOn occasion there might be information not suitable to be made public. In this situation, messages to the reviewer(s) via an external messaging system such as Discord, Slack, Teams, etc. should be used. This might take the form of providing the additional information in a channel or via direct messages to the individual(s) who will be performing the review.\nUsing external messaging systems has the additional benefit of being able to discuss the change in a private, (a)synchronous way rather than relying on the email being sent with comments on from the Git provider. Relying on the Git provider to send emails can break the flow and mean synchronous communication isn\u0026rsquo;t possible.\nA downside to using external messaging systems is that potentially pertinent information is not available to people who might want to be involved in the reviewer but are not included in the other channels.\nPersonal experience In my experience, a mixture of providing base contextual information within the PR\u0026rsquo;s description is always useful. Along with additional information provided to the reviewer(s) via the messaging system of choice.\nI also typically use a messaging system to discuss details about potential changes rather than having a conversation over PR comments. However, this does vary if I think it would be useful to have the discussion in public forum where I or others can refer back to in the future.\n","permalink":"https://blog.st3v3nhunt.me/posts/communicating-changes-in-prs/","summary":"\u003cp\u003eWhen asking for a review of a pull request (PR) ensure there is suitable\ncontext provided to the reviewer(s).\u003c/p\u003e","title":"üîä Communicating changes in PRs"},{"content":"A common architectural style (at the time of writing) is to split up applications into many, smaller, more focussed applications. Frequently this style is known by the term microservices. When working with microservices I\u0026rsquo;ve rarely had the need to run, and actually use, more than a single web application concurrently in my development environment.\nI\u0026rsquo;m using the term web application here to mean a web site as opposed to an API with an HTTP interface. A major difference being that a web site will use cookies for authentication, saving session data, preventing Cross Site Request Forgery, etc. And, although a web based API will likely need some of the same features these are often handled by a separate layer such as an API Gateway.\nFor systems with several web apps wishing to be run locally, at the same time, an issue can occur with cookies clashing. Cookies are specific to domains and paths. The key name against which values are stored can be different, however, for sites using libraries to handle cookie configuration it is common for the key to be the same. Therefore, it is likely different web apps running on the same host will end up reading and writing the same values. This causes issues with unexpected values being present, often resulting in the web app breaking.\nRunning the apps on different ports e.g. http://localhost:5000, http://localhost:8080, doesn\u0026rsquo;t help as cookies have no notion of port.\nThere are several ways to work around this. Perhaps the easiest option, for situations with two sites is to access one site using localhost and the other site with 127.0.0.1. Doing so provides each web app with its\u0026rsquo; own cookie and fixes the cookie clashing issue.\nHowever, for situations with more than two web apps a different approach is required. A good option is to create domain names on the development machine, one for each web app. To do so, entries can be added into the machine\u0026rsquo;s hosts file e.g.\n127.0.0.1 admin-web.localhost 127.0.0.1 super-user-web.localhost 127.0.0.1 user-web.localhost This results in being able to access as many different web sites as is required. Additional benefits to using this approach include:\nPassword managers will store each domain separately, preventing many, many entries accumulating for localhost. An added bonus is deciding which password to use is much easier if there is only a single choice! The name of the subdomain can be meaningful, potentially mapped to the name of the code repo, helping understand which code is running which site. Ports become unnecessary for differentiating between web apps. Other web based applications that run locally can have bespoke subdomains created for them e.g. RedisInsight and pgAdmin. It is worth noting the domain name does not need to be a subdomain of localhost, however, doing this is useful as localhost is a reserved domain name and is treated in a special by many applications. This includes web browsers which will alert you of accessing sites that are not secure unless they are served from the localhost TLD.\n","permalink":"https://blog.st3v3nhunt.me/posts/app-specific-domain-names-for-local-dev/","summary":"\u003cp\u003eA common architectural style (at the time of writing) is to split up\napplications into many, smaller, more focussed applications. Frequently this\nstyle is known by the term \u003ccode\u003emicroservices\u003c/code\u003e. When working with microservices\nI\u0026rsquo;ve rarely had the need to run, and actually use, more than a single web\napplication concurrently in my development environment.\u003c/p\u003e","title":"üñ•Ô∏è  App specific domain names for local dev"},{"content":"Following on from the post about development practices and principles I wanted to capture principles more related to general ways of thinking and working. As before, this isn\u0026rsquo;t in any particular order.\nBreak work down into as small chunks as it practicable If there is a large piece of work to be done, breaking it down into smaller chunks is generally a good idea. The chunks should be as small as makes sense within the context of where and how the work is happening. This is pretty much the idea behind the adage of trying to eat an elephant one bite at a time.\nThere are both benefits and considerations (not necessarily cons) for chunking up work. The benefits of smaller chunks of work include:\nTaken on their own, they are less complex It is easier to understand the dependencies between the chunks and work out which can be done in parallel, ideally leading to the ability (if there is the capacity) for the overall piece of work to be completed more quickly Having a unit of measurement i.e. a chunk of work from which a forecast can be generated (once there are some complete chunks) for remaining effort Identifying points to reflect on and assess what has been achieved, the value gained, whether it is worthwhile continuing, etc. are possible whereas if the work is a single thing that isn\u0026rsquo;t possible until the end Considerations for chunking up the work include:\nBreaking work into chunks is more art than science and as such it isn\u0026rsquo;t going to be \u0026lsquo;correct\u0026rsquo; (as there are several ways of doing it and it is likely going to vary by when it is done, by whom, what they know at the time, etc.) Overall, the complexity of the work is likely to remain, it is just going to be moved around and its nature might change. The individual pieces of work should be simpler but there are likely going to be dependencies between them that will need to be managed and understood Consider what isn\u0026rsquo;t there as much as what is I think of this as a good practice mainly when reviewing code changes but it is much more applicable than that.\nWhen I\u0026rsquo;m doing code reviews, along with reviewing the things that have changed, I try to think about what hasn\u0026rsquo;t changed.\nIs there something that should have been changed? Perhaps some documentation needs to be updated or maybe another system is relying on some part of the application\u0026rsquo;s interface that isn\u0026rsquo;t immediately obvious as an external interface. The format of a generated file picked up by another system perhaps? Of course, in an ideal world this type of change would have a thorough and robust set of tests to ensure changes to it were flagged up. However, that isn\u0026rsquo;t always going to be the case and there is some stuff that just won\u0026rsquo;t be possible to test (at least with a reasonable cost). Understanding or at least having knowledge of the implications for changing systems beyond the system itself can be very valuable.\nAn indicator of not thinking like this can be an immediate approval of a code review. Whilst a very quick review is sometimes appropriate and can be welcome (after all, it means the code was good, doesn\u0026rsquo;t it?!). I do worry it hasn\u0026rsquo;t been given the appropriate level of scrutiny it deserves.\nWhen reviewing code I always feel a little disappointed in myself if I can\u0026rsquo;t find at least one thing to comment on.\nMaximise work not done I\u0026rsquo;m not sure everybody would agree with this sentiment but I think it is important to maximise the work not done. By which I mean I only want to be doing work that is valuable, whether that is for myself or for my employer. It is a good job my definition of valuable is quite broad! Ideally the goal of what is being done is to learn something new or improve current understanding.\nI place little value in being busy for the sake of being busy (there are times when this is OK but they are fairly limited). An example would be if there is a manual activity being repeatedly done, it should ideally be automated. Another example might be if somebody has a desire to analyse some organisational data from several data warehouses, it can be easy to just ask for all of it. If the effort to do that is the same it isn\u0026rsquo;t a problem. However, if the effort isn\u0026rsquo;t the same, the objective of the analysis needs to be more well defined. It might be that not everything is required and the effort can be reduced. Asking questions like this can also help to tease out the essence of the activity and understand the value of it.\nThe question is, \u0026lsquo;How do you maximise the work not done?\u0026rsquo;.\nI think techniques to understand the answer to the question work best when dealing with software as it is often relatively easy to figure out ways to learn more about the problem in safe ways and ways that are very close to the actual problem. The same is much harder to achieve in the physical world, although it is possible to learn what people want. Desire paths (aka lines) are a good example of this.\nWith software, opportunities are much easier to come by. An example I read and will always remember the moral of (if not the details) was about a company selling something like bird cages (it doesn\u0026rsquo;t really matter what it was).\nThe story goes along the lines of somebody wondered if there was a market for bird cages in their home country. They knew there was a big market in other countries but hadn\u0026rsquo;t seen any sellers in their country. In order to figure out if anybody was interested in buying bird cages they created an advertisement on a search engine with a contact form, not really expecting it to do much. Some short period of time later they were capturing a lot of contacts so they knew there was a market and could afford to invest in it. At this point the cost had been minimal and they had a database of people who were wanting to buy a product they didn\u0026rsquo;t have. A good position to be and much better than having a load of stock with no customers. They got in touch with the manufacturers and were able to get stock for their customers. There was a lot of manual effort to start with but they had a business they were able to make money from and grow.\nI like this story as it epitomises figuring out a very low cost, low effort way to learn if there was any demand from which a successful company was created. Very much maximising the work not done (until it was required). It would have been very different if the company had been setup, created a website for order fulfilment, got the manufacturers and shipping sorted without any income or knowledge whether there was a demand.\nYou build it, you run it I subscribe to the idea of \u0026lsquo;if you build it, you run\u0026rsquo;, from a software product perspective (there are caveats to this and the context is all important). It goes hand in hand with thinking about products as opposed to projects and intimately ties two frequently separated stages of a product\u0026rsquo;s life cycle (development and operating) together. Doing so brings some keys benefits such as:\nConsistency, an existing knowledge base and understanding of the history of the product A desire to make the product as good as possible as opposed to running somebody else\u0026rsquo;s thing A much reduced chance of rework so it fits in with the other products the operations teams manages There isn\u0026rsquo;t anybody else to point at when things break These benefits all add up to providing a better product for the users. Which isn\u0026rsquo;t to say you can\u0026rsquo;t have great products that are built and operated by different teams of people but I believe there needs to be a clear lineage and relationship between the two groups for the best experience. And it tends to be easier to do that when it is the same team.\nDo not outsource your key competencies Lots of organisations outsource work and there are lots of good reasons to do so. When the work is mundane and not a reason why you are in business it makes a lot of sense. It might be something like a bakery buying accounting software or a plane maker employing a security contractor. This make sense, neither of those businesses are outsourcing the reason they have a business.\nIt becomes odd when you hear about businesses outsourcing their own business. For example, a business providing specialist recruitment services should not be outsourcing recruitment services to external parties. If you are paying an external entity to do your business you are losing out on all of the learning you should be doing about your business. This is effectively paying somebody to learn about what makes your business a business.\nIn most cases that isn\u0026rsquo;t going to turn out so well. That might happen quickly or it might be a slower decline where you are being put in a position where your competitors, at least the ones who have been learning about the business will be able to out manoeuvre you. Something you do not want that to happen.\n","permalink":"https://blog.st3v3nhunt.me/posts/general-working-principles/","summary":"\u003cp\u003eFollowing on from the post about\n\u003ca href=\"../principles-dev/\"\u003edevelopment practices and principles\u003c/a\u003e\nI wanted to capture principles more related to general ways of thinking and\nworking. As before, this isn\u0026rsquo;t in any particular order.\u003c/p\u003e","title":"üí≠ General working principles"},{"content":"During my time working in software development I\u0026rsquo;ve picked up a few guiding principles I try to consistently follow. Some of them have always been the way I\u0026rsquo;ve tried to work and others have been adopted after having been burnt by not using it in the first place or learning better ways of doing and thinking about things.\nAs with almost everything, the context around which I am working impacts on how closely the principle is able to be implemented but I try to apply them as often as is appropriately possible.\nThe principles in this post are closely related to software development activities and are in no particular order, although I have tried to group similar ones. It is by no means an exhaustive list of the principles I follow. I will aim to add to the post every so often as I recall principles and discover new ones. The definition of the term principle will be stretched here and there to encompass guidelines and practices too.\nThis is likely to be the first in a series of posts on ways of working and thinking I tend to subscribe to.\nBe cautious using latest Whether a Docker image, Helm chart, code dependency or pretty much anything else you care to think about, using the latest version of it needs to be carefully considered. If the use of latest is simply a way of selecting which version of the thing is going to be used before whatever specific version is locked down that is fine e.g. during initial and active development of something. However, using the latest in situations where the thing will not be locked to a specific version and is resolved at build, deploy or runtime is not a good idea.\nWhilst using latest might seem like an OK thing to do at face value, and indeed, there are some situations where it is perfectly OK. Situations such as initial development and general trialing of something. However, most situations should require a specific version to be recorded and used. The reason being is that when using latest you are not only putting trust in the creator of the thing to have correctly working tagging practices but trusting the creator\u0026rsquo;s ability to maintain access and security controls.\nUsing latest, in some respects makes a bit of a mockery of testing. If you are testing against the latest version of something what have you actually tested? Is the latest version going to be the same tomorrow or next week or next month?\nAlways increment MAJOR version on any breaking changes Whenever a breaking a change is made to a public interface the MAJOR version should be incremented. Regardless if the aspects of the interface you thought should have been private were the only changes. There is a possibility they were being used and if they were changed without a breaking change version increment the users of them will not be happy. The principle of \u0026lsquo;if it is available, it will have been used\u0026rsquo; should be taken to heart and an expectation that there will be users of it.\nTreat any API the same as you would a web API By that I mean, if there is an API that is accessible, whether it was meant to be accessible or not, it should be treated as such. Consequentially, when breaking changes are made MAJOR versions should be incremented to indicate the change.\nThe rule of three When something has been done for a third time or you know it will be done or used at least three times it is worth considering doing it in such a way that it will be reusable. That might entail refactoring some code into a shared library or module or it might (in relation to the point about automating as much as possible, below) mean that the activity should be automated.\nThe rule of three is useful for a couple of reasons. If something has been or will be used three times it is likely:\nThere is something within it that is generic enough that it will be possible to extract it into something that is useful to more than three things. However, the context needs to be borne in mind to check this will be the case The details of the reusable bit will be better understood and enable the useful bit to be identified and hopefully the majority of the changes that would be required in future uses of it Use automated release note generation Something like semantic-release can be used to automatically create release notes and correctly increment versions. Ideally an automated release note will satisfy all requirements for all intended audiences. However, if that isn\u0026rsquo;t the case, having a release note that is already created will be much easier to add to than one that needs to be created from scratch.\nAutomate as much as possible, whenever possible Automating as much as possible is a good reason for the purpose of not only having a system that can perform the task at hand quicker than a human, with fewer mistakes than a human, it also has the major benefit or codifying the task.\nCodification of the task is a huge benefit as it means it doesn\u0026rsquo;t need to be maintained in the memory of humans or written down in a non-executable form (that will become incorrect at some point and ultimately cause confusion as the humans will not know which is correct). Codifying tasks effectively delegates the memory storage of the details.\nMuch like the rule of three (above), the task being automated should be assessed as to whether it will repay the investment in the automation before blindly trying to automate it. Some tasks are complex, can be automated easily and are run many times. However, there are other tasks that are simple, take significant effort to automate and run infrequently. With the majority falling between these extremes. There really isn\u0026rsquo;t a golden rule as to when a task should be automated as it very much depends on the context.\nIt is worth mentioning that there is the additional benefit of the codification of the task to take into consideration when deciding whether to automate. If a task is run infrequently it might very well be the case it is difficult to recall the steps and it would be beneficial if it could just be run without having to recall the details, and having those details captured in an executable form..\nUse a linter Set a standard for the syntax of the language being used and, as much as possible (really, it should be a never event), do not change it. Using a linter to validate and prevent syntax issues being part of code review discussion has a number of benefits:\nQuicker feedback on issues, no need to wait for a review More consistent code as linter will not miss discrepancies like a human would No time needs to be spent on bike-shedding Reviews can focus on important issues Health checks Have some way of accessing key info about what you are building, whether that is a web frontend or a backend microservice. Something like a health check (as used by Kubernetes) with key information about the thing e.g. the deployed version, memory use, CPU user, uptime, concurrent users, etc.\n","permalink":"https://blog.st3v3nhunt.me/posts/dev-principles-and-practices/","summary":"\u003cp\u003eDuring my time working in software development I\u0026rsquo;ve picked up a few guiding\nprinciples I try to consistently follow. Some of them have always been the way\nI\u0026rsquo;ve tried to work and others have been adopted after having been burnt by not\nusing it in the first place or learning better ways of doing and thinking\nabout things.\u003c/p\u003e","title":"‚ú® Development practices and principles"},{"content":"I watched When To Use Microservices (And When Not to!) soon after it was published but never got round to publishing this post, which I wrote at the same time but have edited more recently so I could publish it. The post contains some thoughts I had about the video and thought it would be worth capturing them and maybe try to get back into blogging at some regular interval, although clearly that didn\u0026rsquo;t happen at the time\u0026hellip;\nTL;DR The video is relatively short and worth the time to watch. It is an overview and comparison or microservices and monoliths. Sam does a good job of highlighting the difficulties and challenges around \u0026lsquo;doing\u0026rsquo; microservices, provoking thought on the subject.\nI\u0026rsquo;ve not seen a GOTO video in this style before (and as it turns out five months later I\u0026rsquo;ve not seen another like it either). I think I like it. It certainly makes the video easier to digest in chunks and provides a breakdown of the key takeaway points which can be useful when reviewing the video yourself or discussing with others.\nMy Commentary In order to provide some structure, I\u0026rsquo;ll cover the different themes from the video.\nChange is Hard I totally agree with the sentiment of how the people side of change is really hard and actually making things change, in a lasting way, is incredibly tough. Without having any empirical data I would imagine change is likely doomed to fail most of the time. Perhaps in a similar vein to how a gambler only ever tells you about their success people only ever tell you about successful changes\u0026hellip;\nFrom my experiences, I think changing the ways of working within an organisation needs to originate from where the power-base resides. By power-base I mean the group of people (sometimes an individual) that can veto decisions and has the final say on whether something is going to happen or not. In some organisations that will be product development team(s) and they will be able to tread a path that will be followed by others. However, I think for a lot of organisations the power resides in the operational teams. If change is attempted without the operational team either leading it, or at the very least being heavily involved, the change is likely going to fail. I\u0026rsquo;ve seen product development teams attempt to make changes with the hope that others will follow, however, as it is ultimately the operations teams that are responsible for the running of the product, the change hits a hard stop at the boundary of the product development team\u0026rsquo;s sphere of influence.\nOf course there are other power-bases within organisations but it often ends up being held by a small number of people or perhaps a individual. Some organisations operate by diktat and when one arrives from on high, that can force change. However, the forcing of change is generally not an effective way to achieve change. It may achieve rapid change in the short term but there are likely longer term issues having the ground work laid to causes problems in the future. Forced change also risks getting exactly what was asked for as opposed to being the change that the sentiment intended, thus loosing the benefits of making the change in the first instance.\nIn my experience, achieving change is much easier when the change is being instigated by the people that build the thing and run the thing. The reason this works is that the sphere of influence of that team encompasses everything where change will happen.\nJust how many organisations have this setup? I wish it were more but there are many reasons why it isn\u0026rsquo;t common. One of them being the idea of dev-ops is much newer than the how long organisations have been around and changing an organisation from one where development and operations are separate to one where they are working together, even moderately so is yet more change and as we know now, change is difficult. Sam touched on it briefly towards the end of the video, asking a team of people who have never been on call to start doing it isn\u0026rsquo;t as easy as it sounds. Often the financial reward isn\u0026rsquo;t commensurate with the sacrifices (either perceived or real) required. In this case it can boil down to whether the person really cares about the thing they are working on, in some cases that isn\u0026rsquo;t going to be a problem but in others it most certainly will be. And of course going on call may well require contracts to be renegotiated which be tricky.\nI\u0026rsquo;ve thought about this issue on and off in the time between drafting and publishing this post. I\u0026rsquo;m at the stage of believing a lot of the problems around change are due to the culture of an organisation along with the structure. If an organisation\u0026rsquo;s culture is one built on minimising change then it is going to be much more difficult to make meaningful and lasting change. If the culture is one of adaptability and embracing change then making internal changes is likely going to be easy. I\u0026rsquo;ll get round to writing more about organisation culture at some point.\nUsing Microservices Touching on the reasons for using microservices, it was interesting how Martin picked up on highlighting the business benefits of having independent deploy-ability such as lowering risks and providing business units/product teams with more autonomy (linking into the communication interfaces of APIs) rather than focussing on having this often nebulous ability to deploy things independently. Deploying with zero down time is interesting as although it has a business benefit (and an easy non-functional requirement to say is a must), it depends on the business as to how much value it really is.\nThinking about how this applies to my experiences, I\u0026rsquo;ve seen really robust, slick deployment capabilities be somewhat wasted as the \u0026lsquo;continuous delivery\u0026rsquo; pipeline stopped before deploying to the production environment. And the production environment deployment was handled in a totally different manner.\nIf the continuous delivery pipeline doesn\u0026rsquo;t end in delivering the product to the production environment I\u0026rsquo;m no sure how accurate it is to call it continuous deliver. Maybe if it is contextualised e.g. \u0026lsquo;a continuous delivery pipeline to staging\u0026rsquo; (or whatever is the last environment) at least this makes it clear it isn\u0026rsquo;t a complete delivery pipeline.\nThis brings me onto deployments and how things are actually done.\nDeployments A major benefit to microservices over monoliths is their independent deploy-ability. However, I\u0026rsquo;d argue this doesn\u0026rsquo;t matter if the deployments aren\u0026rsquo;t done frequently (resulting in many changes being bundled together across many microservices) or they are done in such a way that all microservices within the larger system are deployed in one go. This might have been what was referred to in the video when Sam mentioned an organisation that had been able to deploy independently at some point but never did it and as a result lost the capability.\nWhy bother with the cost of the flexibility of being able to do something if the reality is that it is never utilised. It is analogous to buying a vehicle with seven seats and loads of luggage space and only ever using it to drive yourself around. It might be really flexible but the reality is not only did it cost more to purchase in the first place there is an ongoing cost of maintaining the larger vehicle.\nThis leads onto the mention of the cost of things.\nCosts of Doing Things It was good to hear a bit of discussion about thinking about the costs of doing things. It seems really common for solutions to be chosen because they could solve everything and could be extended to cater for all eventualities but the reality is that there is a cost involved in having that level of flexibility and most of the time this isn\u0026rsquo;t thought about too much.\nWhat is the cost of having a system be so generic and abstract when trying to develop things or maintain those things or run those things? Where does the domain knowledge reside? Does it get spread throughout multiple systems with different teams being responsible for the different systems?\nI\u0026rsquo;ve seen a system just like this go from a configurable master-piece where no development resource was ever going to be required to something that needed development input regularly as edge cases were discovered and the complexity of the system became overbearing. The result being a very costly system in every aspect of its life cycle, from design to development to maintenance. To the ability to understand the implications of changes to it and by it. Not the way to go.\nCommunication via APIs/Microservices Linked to the challenge of making changes is difficulties in communication across large groups, mainly because people are people. I\u0026rsquo;m very much a proponent of providing interfaces between teams in the form of APIs and microservices as it leaves no room for interpretation, at least that is the idea, obviously it doesn\u0026rsquo;t always work in practice.\nSummary This post is quite long and wordy, fortunately I\u0026rsquo;ve added a TL;DR which can be reduced further to just recommending the video is watched. Along with stating that context is everything when deciding whether to use microservices or not.\n","permalink":"https://blog.st3v3nhunt.me/posts/when-to-use-microservices/","summary":"\u003cp\u003eI watched\n\u003ca href=\"https://www.youtube.com/watch?v=GBTdnfD6s5Q\"\u003eWhen To Use Microservices (And When Not to!)\u003c/a\u003e\nsoon after it was published but never got round to publishing this post, which\nI wrote at the same time but have edited more recently so I could publish it.\nThe post contains some thoughts I had about the video and thought it would be\nworth capturing them and maybe try to get back into blogging at some regular\ninterval, although clearly that didn\u0026rsquo;t happen at the time\u0026hellip;\u003c/p\u003e","title":"üî¨ When to Use Microservices (And When Not to!)"},{"content":"GitHub is a pretty decent cloud based Git repository. I would put it in my top 2 along with GitLab.\nHowever, I have a love/hate relationship with the notifications. In so far as I hate them. I find the emails are unreliable, sometimes they come through quite quickly and on other occasions they seem to take an age and I\u0026rsquo;m pretty sure I\u0026rsquo;ve not received some at all.\nI\u0026rsquo;ve tried the web notifications and as far as I can tell they are pretty useless unless you have a client that will read them and notify you. The notification UI on the site is OK but I\u0026rsquo;m not interested in having the webpage open and checking it manually. Perhaps there are some simple ways of working with them that I\u0026rsquo;ve not come across\u0026hellip;However, I have tried using GitHub notifier clients such as Gitify for the desktop and this Chrome extension but I still don\u0026rsquo;t feel I am getting what I want from the notifications. No fault of the clients, they seem to work OK.\nIf the clients tell me when things happen, what else is it that I want? I think my problem is I have been spoilt in the past as I have used the GitHub integration with Slack to pump notifications into a dedicated GitHub channel. This has provided me with the insight into what I think it missing - the record of events. It appears to me that the aforementioned notifiers do little more than read the central GitHub list of notifications and mark them as read. And when a notification is read it goes, poof, never to be seen again. Well, not until the all notifications page is viewed. So whilst the functionality exists, as already mentioned I don\u0026rsquo;t want to have the page open, manually checking on it - it just feels a little unloved.\nThe good news is using Slack as my GitHub notification hub is a really great experience, it truly works quite brilliantly. The only slight downer is that I tend to belong to Slack communities that are on the free tier and the GitHub integration is less useful than other integrations. So what to do - the solution is really quite simple. I have my own Slack domain and I use one of the limited integrations for GitHub. Now I can configure which repos I get notified about, what level of detail I have, all to my heart\u0026rsquo;s content. And I don\u0026rsquo;t need to worry about bombarding others with GitHub notification overkill as there is only one in my own community! With Slack powering how I get notified I have all of the configurability Slack is known for across the many Slack clients there are.\n","permalink":"https://blog.st3v3nhunt.me/posts/use-personal-slack-for-github-notifications/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/\"\u003eGitHub\u003c/a\u003e is a pretty decent cloud based\n\u003ca href=\"https://git-scm.com/\"\u003eGit\u003c/a\u003e repository. I would put it in my top 2 along with\n\u003ca href=\"https://gitlab.com/\"\u003eGitLab\u003c/a\u003e.\u003c/p\u003e","title":"üè¶ Use personal Slack for GitHub notifications"},{"content":"Now I have had the TeamCity build pipeline running for a while I have had chance to review how it works. I have made a number of changes which I have summarised, along with the reasoning below.\nAdditional build step Originally there were 6 build steps, now there are 7.\nThe new build step is executed as the first step and its purpose is to prevent the build from continuing if a History Build is detected. The way I detect a history build is to check the SHA-1 of the default branch and compare it to the SHA-1 being built.\nThis is not quite correct as far as the TeamCity definition goes for a history build but it works in order to prevent old code being packaged into the NuGet packages. This was happening whenever a branch was deleted as it was merged into the default branch.\nThere is a check to make sure the build is not for the default branch so the build will only be stopped when the code being built is the same as that in the default branch but it is not the default branch that is being built. This has the impact of branches that contain the same code as the default branch will not be built, however, we felt the trade off is worth it.\nThe script used is ostensibly the same as this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026lt;# Params: $current_build_sha = \u0026#34;%build.vcs.number%\u0026#34; $root_branch = \u0026#34;%vcsroot.branch%\u0026#34; $is_default_branch = \u0026#34;%teamcity.build.branch.is_default%\u0026#34; $vcs_root_url = \u0026#34;%GIT_ROOT_URL%\u0026#34; $private_token = \u0026#34;%GIT_PRIVATE_TOKEN%\u0026#34; e.g. CheckForHistoryBuild.ps1 \u0026#34;%build.vcs.number%\u0026#34; \u0026#34;%vcsroot.branch%\u0026#34; \u0026#34;%teamcity.build.branch.is_default%\u0026#34; \u0026#34;%GIT_ROOT_URL%\u0026#34; \u0026#34;%GIT_PRIVATE_TOKEN%\u0026#34; #\u0026gt; param([String]$current_build_sha, [String]$root_branch, [String]$is_default_branch, [String]$vcsroot_url, [String]$private_token) If ($is_default_branch -eq $true) { Write-Host \u0026#34;Branch is default. Pipeline continuing...\u0026#34; exit 0 } Write-Host \u0026#34;Get SHA-1 of the project\u0026#39;s default branch ($default_branch_name) in order to compare to this build\u0026#39;s SHA-1\u0026#34; Write-Host \u0026#34;If they are the same the build will not be continued\u0026#34; Write-Host \u0026#34;This is due to the build being a History Build (https://confluence.jetbrains.com/display/TCD9/History+Build)\u0026#34; Write-Host \u0026#34;And history builds create NuGet packages with old code\u0026#34; $default_branch_name = ($root_branch -split \u0026#34;/\u0026#34;)[-1] $api_url = \u0026#34;https://$vcsroot_url/api/v3/projects/55/repository/branches/$default_branch_name`?private_token=$private_token $default_branch_api_response = Invoke-RestMethod $api_url $default_branch_sha = $default_branch_api_response.commit.id Write-Host \u0026#34;Call API @ $api_url\u0026#34; Write-Host \u0026#34;Default branch name: $default_branch_name\u0026#34; Write-Host \u0026#34;Master branch SHA-1: $default_branch_sha\u0026#34; Write-Host \u0026#34;Current branch SHA-1: $current_build_sha\u0026#34; If ($default_branch_sha -eq $current_build_sha) { Write-Host \u0026#34;History build detected. Stopping pipeline...\u0026#34; exit 1 } Else { Write-Host \u0026#34;New build. Pipeline continuing...\u0026#34; } I have noticed the call out to the GitLab API is failing quite frequently. I will looking into making it more robust in the future but it can wait for the time being seen as the worst thing that happens is the history builds are not detected. Hardly ideal.\nRewritten NuGet package step I have rewritten one of the original steps which I have already written about regarding the creation of the NuGet packages. Since then I have made more changes to the script. The changes are mostly due to it now being a standalone script and not source code saved in TeamCity. The other changes are that a datetime stamp is no longer appended to the package version as it isn\u0026rsquo;t necessary, the pattern -prerelease is sufficient to indicate a pre-release NuGet package. The final change being to ignore errors during the deletion of the output directory. The new script is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026lt;# Params: $working_dir = \u0026#34;%teamcity.build.workingDir%\u0026#34; $nuget_packages_dir = \u0026#34;%Generated nuget packages directory%\u0026#34; $is_default_branch = \u0026#34;%teamcity.build.branch.is_default%\u0026#34; $build_counter = \u0026#34;%build.counter%\u0026#34; e.g. CreateNuGetPackages.ps1 \u0026#34;%teamcity.build.workingDir%\u0026#34; \u0026#34;%Generated nuget packages directory%\u0026#34; \u0026#34;%teamcity.build.branch.is_default%\u0026#34; \u0026#34;%build.counter%\u0026#34; #\u0026gt; param([String]$working_dir, [String]$nuget_packages_dir, [String]$is_default_branch, [String]$build_counter) $output_dir = \u0026#34;$working_dir\\$nuget_packages_dir\u0026#34; $properties = \u0026#34;Configuration=Release\u0026#34; Write-Host \u0026#34;Default branch detected: $is_default_branch\u0026#34; Write-Host \u0026#34;Build counter: $build_counter\u0026#34; Write-Host \u0026#34;Cleaning output directory: $output_dir\u0026#34; rm \u0026#34;$output_dir\\*\u0026#34; -Recurse -ErrorAction Ignore If ($is_default_branch -eq $true) { Write-Host \u0026#34;Package is versioned as release.\u0026#34; $package_version = \u0026#34;1.0.$build_counter\u0026#34; } Else { Write-Host \u0026#34;Package versioned as pre-release.\u0026#34; $package_version = \u0026#34;1.0.$build_counter-prerelease\u0026#34; } ..\\..\\tools\\NuGet.CommandLine.DEFAULT.nupkg\\tools\\NuGet.exe pack $working_dir\\MySolution\\MySolution.csproj -OutputDirectory $output_dir -Version $package_version -Properties $properties -IncludeReferencedProjects Moved PowerShell script out of TeamCity I was saving the PowerShell script directly in TeamCity but have moved the code into a file and added it to the code repository. I find having the script alongside the code is a much better option, some of those reasons being:\nAnybody with access to the code has access to the build script (which is often different to those with access to the build configuration) The script is fully versioned The same scrutiny as the rest of the code gets is applied to the script Changes to the script are applicable only to the branch where those changes are rather than everything that runs through the build configuration - this is perhaps the most important reason In the future I would go straight to having the build configuration reference a script within the code base as the transition from one to the other is tricky. Mostly involved with getting the script to work properly for the first time whilst all builds are running, some of which do not have the build script in the repo.\nUpdated failure conditions I started with four failure conditions and reduced that to three, of which only two are from the original four.\nThe two I have disabled are the build duration and the artifact size. The reason being, both were failing builds that were perfectly OK. I thought this might be the case in the early days of the project as the code can and indeed does change quite significantly from build to build.\nThe new failure condition goes hand in hand with the script to stop the build when a history build is detected. The condition is triggered when the build log contains some text that is output via the build script when the build has been identified as one to be stopped.\nMore changes? For the time being those are the changes that have been made. As and when there are more I will update this post or create a new one.\n","permalink":"https://blog.st3v3nhunt.me/posts/teacmity-pipeline-iteration-two/","summary":"\u003cp\u003eNow I have had the \u003ca href=\"../teamcity-pipeline/\"\u003eTeamCity build\npipeline\u003c/a\u003e running for a while\nI have had chance to review how it works. I have made a number of changes which\nI have summarised, along with the reasoning below.\u003c/p\u003e","title":"üéâ TeamCity Pipeline - iteration two"},{"content":"As I was reviewing the TeamCity build configuration I\u0026rsquo;d setup with my colleagues, it was pointed out every build was creating a new NuGet package. Not great when you consider every branch was triggering a build and most of those branches were not master.\nThe problem with every branch Generally it is not a good idea to have every branch generating a NuGet package when those branches are often feature branches i.e. code that is in the process of being reviewed but has not yet been merged into master. These were not changes we wanted to be published into the NuGet feed and made available to consumers by default.\nAs I was thinking about ways this could be solved I recognised what I really wanted was a build step controlled by some (probably environment/execution context) variable. Looking around the Internet to see what other people did when faced with this situation I found a feature request ticket for exactly this - Execute a build step based on a condition. The ticket is over 4.5 years old so whether the feature will be implemented remains to be seen (one would think not).\nThinking about why this seemingly innocuous feature might not have been implemented, it struck me that build pipelines should be all about procedural steps that produce the same outcome each and every time they are executed. This is what builds (no pun intended) trust in the process and allows people to rely on them. Adding conditional logic is adding something that might go wrong, and things often go wrong when you least want them to (and have forgotten about the condition even existing)!\nPossible solutions In the ticket a number of commenters share their workarounds.\nSome people create additional build configurations and use VCS triggers to control which branch goes through which build configuration. Others look to leverage the shell and environment variables to control what happens within a single build configuration. Thinking about the implications of the two options I decided to use a PowerShell script to control the generation of the NuGet package. I chose this option primarily due to GitLab only supporting a single TeamCity build configuration and I wanted to maintain the integration between GitLab and TeamCity.\nNuGet creation via PowerShell The PowerShell script I ended up with is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $output_dir = \u0026#34;%teamcity.build.workingDir%\\%Generated nuget packages directory%\u0026#34; $properties = \u0026#34;Configuration=Release\u0026#34; Write-Host \u0026#34;Cleaning output directory: $output_dir\u0026#34; rm \u0026#34;$output_dir\\*\u0026#34; -recurse If (\u0026#34;%teamcity.build.branch.is_default%\u0026#34; -eq $true) { Write-Host \u0026#34;Package is versioned as release for branch: %teamcity.build.branch%\u0026#34; $package_version = \u0026#34;1.0.%build.counter%\u0026#34; } Else { Write-Host \u0026#34;Package versioned as pre-release for branch: %teamcity.build.branch%\u0026#34; $patch_name = Get-Date -UFormat %%Y%%m%%d%%H%%M%%S $package_version = \u0026#34;1.0.%build.counter%-pre$patch_name\u0026#34; } ..\\..\\tools\\NuGet.CommandLine.DEFAULT.nupkg\\tools\\NuGet.exe pack %teamcity.build.workingDir%\\MySolution\\MySolution.csproj -OutputDirectory $output_dir -Version $package_version -Properties $properties -IncludeReferencedProjects The logic is straight forward enough, if the branch is determined to be the default one, create a package based on the build number e.g. 1.0.12. If the branch is not default, a pre-release version will be created with the date and time the package was created e.g. 1.0.11-pre20160418093024. The reason for prepending the time stamp with pre is to make the version number valid due to the pre-release section not able to be purely numeric. Another change I had to make to the way I was originally going to set the pre-release section (based on branch name) was due to the length limitation of 20 characters max. The time stamp is a fixed length, until we get to the year 10,000 AD at least.\nTaking this approach allows all branch builds to be visible in the NuGet feed but only those built from master are not marked as pre-release. This means as we develop the package we can select specific pre-release versions to test before merging them into master, should we wish.\n","permalink":"https://blog.st3v3nhunt.me/posts/branch-based-nuget-packages-in-teacmity/","summary":"\u003cp\u003eAs I was reviewing the TeamCity build configuration I\u0026rsquo;d\n\u003ca href=\"../teamcity-pipeline/\"\u003esetup\u003c/a\u003e with my colleagues, it\nwas pointed out every build was creating a new NuGet package. Not great when\nyou consider every branch was triggering a build and most of those branches\nwere not \u003ccode\u003emaster\u003c/code\u003e.\u003c/p\u003e","title":"üéã Create branch based NuGet packages in TeamCity"},{"content":"Working with a non-cloud hosted Git remote I often work in environments where the code is centrally stored in TFS. And as I hate, HATE, HATE TFS I have to find other ways to interact with the VCS to keep my sanity. GitTFS to the rescue. I\u0026rsquo;ve written about this before.\nI have a number of colleagues who feel the same way about TFS but until recently we were happy to use the shelveset feature to do code reviews. However, after a period of time doing \u0026lsquo;proper\u0026rsquo; reviews in GitHub and GitLab one of my colleagues decided enough was enough and we should do a more Git based code review. I was up for doing that and immediately thought we should get the code into our internal GitLab instance. They didn\u0026rsquo;t want to do that and instead preferred to use their own machine as the remote host. Rather than go into the details of why we decided this was OK I\u0026rsquo;d like to cover the details of the work flow.\nLocal network shared Git repo work flow The following describes how to work with a \u0026rsquo;local\u0026rsquo; (local network share) remote Git repo. This allows a team of developers working with GitTFS to implement a code review system in Git where it is not possible to make changes to the server based TFS system.\nCreate network share On a machine that is accessible by all members of the team a network share needs to be created. This network share should contain the git repo that has been created by GitTFS from the TFS source.\nIn the following example the network share machine is WORKSRV_001 and the location of the GitTFS repo is C:\\code\\my-project\nThe following commands should be run on a machine that is not where the network share is (although this doesn\u0026rsquo;t necessarily matter).\nGet the repo If the directory where the commands are being run is already a Git repo (as it was in my case) add a new remote: git remote add WORKSRV_001 //WORKSRV_001/C$/code/my-project\nFollowed by a check out of the master branch: git cob WORKSRV_001/master WORKSRV_001/master If the directory is not a Git repo, clone it: git clone //WORKSRV_001/C$/code/my-project\nCheckout the branch to review When your colleague informs you there is a branch available for review, that branch needs to be checked out locally so it can be reviewed. git cob review/\u0026lt;branch-name\u0026gt; WORKSRV_001/\u0026lt;branch-name\u0026gt;\nFollow your normal review process and remember to include checking the latest code changes recorded in TFS are contained in the review branch.\nMerge the reviewed branch into a copy of master Once the branch has been reviewed it can be merged into a copy of master. The reason for merging into a copy of master is because the remote is somebodies\u0026rsquo; local Git repo and we don\u0026rsquo;t want to go messing with it! git co WORKSRV_001/master git cob reviewed/\u0026lt;pull-request-description\u0026gt;\nWhen the copy of master is checked out the merge of the reviewed branch can occur. This is done by creating a merge commit (the default behaviour of GitHub and GitLab). To create a merge commit use the following options:\nno fast forward with --no-ff add a message with -m \u0026quot;my commit message\u0026quot; include the previous commit messages with --log git merge --no-ff -m \u0026quot;\u0026lt;message\u0026gt;\u0026quot; --log review/\u0026lt;branch-name\u0026gt;\nCheck into TFS The reviewed and merged branch should now exist on the network shared Git repo. The person whose machine hosts the network share needs to be informed of the branch and that it should be pushed to TFS.\nThe push to TFS involves using GitTFS to check-in the changes: git-tfs checkin -m \u0026quot;\u0026lt;message here\u0026gt;\u0026quot;\nHopefully no additional changes have been made to TFS during the review process. If there are changes, the pusher will need to rebase the reviewed branch against the latest changes (or ask one of the people involved in the reviewed branch to do it) prior to checking it in.\n","permalink":"https://blog.st3v3nhunt.me/posts/diy-git-remote/","summary":"\u003ch2 id=\"working-with-a-non-cloud-hosted-git-remote\"\u003eWorking with a non-cloud hosted Git remote\u003c/h2\u003e\n\u003cp\u003eI often work in environments where the code is centrally stored in TFS. And as\nI hate, HATE, \u003cstrong\u003eHATE\u003c/strong\u003e TFS I have to find other ways to interact with the VCS\nto keep my sanity. \u003ca href=\"http://git-tfs.com/\"\u003eGitTFS\u003c/a\u003e to the rescue. I\u0026rsquo;ve written\nabout this \u003ca href=\"../how-i-work-with-git-and-tfs/\"\u003ebefore\u003c/a\u003e.\u003c/p\u003e","title":"üî® DIY Git remotes"},{"content":"In order to provide a consistent development environment between team members as well as making it easier to do physical pair programming sharing a .gitconfig is a good thing to do. After all, when using another person\u0026rsquo;s machine there are few things more annoying than not having access to the same shortcuts you rely on.\nGit aliases are one of the easier consistencies to bring to a developer\u0026rsquo;s machine. This is how you can achieve a shared .gitconfig.\nAdd a .gitconfig to the repository. This is one I often start with.\n1 2 3 4 5 6 7 8 9 10 11 12 [core] autocrlf = true [alias] st = status hist = log --pretty=format:\u0026#39;%h %ad | %s%d [%an]\u0026#39; --graph --date=short ci = commit com = checkout master b = branch cob = checkout -b co = checkout [push] default = simple Once the .gitconfig is in the repo Git needs to be told to use it.\nTo do that add:\n[include] path = ../.gitconfig To the config file located in the Git directory i.e. .git/config.\nThe manual step not withstanding you now have a .gitconfig all members of the team should be using. It is a good idea to include this information in the project\u0026rsquo;s README.md. And if you were so inclined it could be automated if the project had some setup scripts to run.\n","permalink":"https://blog.st3v3nhunt.me/posts/git-aliases/","summary":"\u003cp\u003eIn order to provide a consistent development environment between team members\nas well as making it easier to do physical pair programming sharing a\n\u003ccode\u003e.gitconfig\u003c/code\u003e is a good thing to do. After all, when using another person\u0026rsquo;s\nmachine there are few things more annoying than not having access to the same\nshortcuts you rely on.\u003c/p\u003e","title":"„ÄΩÔ∏è Git aliases"},{"content":"It has been a while since I setup a build configuration in TeamCity. I thought I would record what I did this time around.\nFirst up, a bit of background - the solution is a pretty simple .Net library. It uses NuGet to fetch some dependencies and I want to package the code up as a NuGet package. I want to make it available in the TeamCity NuGet feed.\nThe source code is hosted in a GitLab instance. I wanted to make use of the GitLab and TeamCity integration so I would be able to see how an MR got on with CI. This would save people time potentially looking at an MR that doesn\u0026rsquo;t even pass CI.\nBuild Steps There are 6 build steps:\nRestore NuGet packages Build with Visual Studio Clean \u0026amp; build the solution in release configuration Run Duplicates Finder: Include **/*.cs Exclude **/*.Tests and **/Properties Run FxCop: Include **/bin/Release/*.dll Exclude **Tests** Run NUnit tests. Include JetBrains dotCover code coverage Package the NuGet Packages. The packages are only made available on the TeamCity feed so there is no Nuget Publish step Integration of CI and VCS I do love an automated check. And those checks are even better when they are done for all MRs before anybody has spent any time eyeballing them. In order to get TeamCity and GitLab to communicate together I did this:\nIn GitLab I went into the project\u0026rsquo;s Settings --\u0026gt; Services --\u0026gt; JetBrains TeamCity CI screen. I completed the form for the TeamCity server I wanted to integrate with. As instructed on the form in GitLab I went to TeamCity and set the Build number format to %build.vcs.number%. That is it. Now all branches trigger a build in TeamCity and MRs have the integrated CI status check.\nCI failure conditions Part of the power of CI is being able to fail the build on pre-defined conditions. For this pipeline I have additional failure conditions as well as the standard of any failing tests and non-zero exit codes. The additional failure conditions are:\nFail build if build duration (secs) is different by at least 50% compared to the last successful build Fail build if artifacts size (bytes) is different by at least 50% compared to the last successful build Fail build if number of ignored tests is more than 0 Fail build if number of tests is less by at least 20% compared to the last successful build Some of those criteria are probably a little on the \u0026lsquo;kind\u0026rsquo; side and will not be suitable for all projects or at all times of their life. For example, having the number of tests reduce by 20% in a mature project is almost definitely far too many, as is the build duration and artifacts criteria. On the other hand, if that project is near to it infancy the build duration might well double if a new suite of tests was introduced.\nFor very young projects it is probably a good idea to disable some of the advanced failure conditions until such a time when the project has some history and predictability.\n","permalink":"https://blog.st3v3nhunt.me/posts/teamcity-pipeline/","summary":"\u003cp\u003eIt has been a while since I setup a build configuration in TeamCity. I thought\nI would record what I did this time around.\u003c/p\u003e","title":"üéÅ TeamCity Build Pipeline"},{"content":"I really enjoy introducing people to Git. Pretty much without fail they see how much better their development work-flow can be and find their work more enjoyable as a result. This is especially true if they are coming from a TFS background. See how I work with Git and TFS. However, with pleasure comes pain. For Git, that pain is in getting over the learning curve. The size and steepness of the curve varies depending on previous experience of Version Control Systems. In my experience, TFS does not provide a good base knowledge to work from as it tries to hide as much as possible from the user. Clearly Git is doing a lot for you too but the way it works is more exposed. For example, even the most basic of things like adding files into the index can seem strange to a TFS user.\nEach time I introduce people to Git I cover most of the same things. I\u0026rsquo;ve used crib sheets in the past but for the most recent round of introductions I thought I would write a post about it. This is that post!\nWhy use Git? I should really do a post specifically explaining the reasons I like to use Git. For the sake of brevity, these are the official reasons.\nBranch name conventions I find it useful to have some conventions for the name of branches that will be pushed to the remote repository. I tend to use:\nfeature/descriptive-name-of-feature fix/descriptive-name-of-fix Naming conventions makes having specific things happening downstream easy e.g. deploy any and only feature branches to a brand new test environment. I wouldn\u0026rsquo;t necessarily want to do that for a fix branch, perhaps that would be deployed over the top of the existing environment. Having known reserved branch names also means people are free to push branches with non-reserved names for their own purposes e.g. backup or sharing.\nCommit message conventions I\u0026rsquo;ve previously written about how I think emojis add to the information contained within a commit message. And how to provide insight into the change being made, as opposed to simply regurgitating the change being made.\nIn addition to the points above I like to have the commit message adhere to the convention of starting with an emoji followed by a relatively short and succinct message. There are a number of good posts discussing this subject in detail, On commit messages and A Note About Git Commit Messages to name two. To summarise, the commit message should ideally have:\nAn emoji to start with! A first line, the summary message, length of ~50 characters A blank line after the summary, when there are additional comment lines Additional comment lines not exceed ~72 characters Commit conventions Good commits should generally:\nInclude a single thing i.e. renaming and fixing are different things and do not belong in the same commit Be as small as possible. If you are ever tempted to just do that really small extra change in a commit - don\u0026rsquo;t. Finish up what you are doing and do the other thing in another commit. You (and your colleagues) will thank yourself when you need to see what changed when that bug was introduced! Git aliases I\u0026rsquo;ve got another post on git aliases and how to share them.\nBranch protection I have found out the hard way, protecting branches is a very good idea. No force pushing to master (or any other shared branch) is an absolute necessity for your sanity.\nMerge/Pull* request assignments When an MR is created you are asked a number of questions. One of them being who you would like to assign it too. The question does not need to be answered immediately, or ever in fact. However, it is a good idea to have the team know how this works in order to avoid the whose job is it anyway problem.\nWorking in a small team the rule I usually work by is to not have the MR creator assign anybody. The first person to get to the MR assigns them self. The problem of nobody assigning themselves can happen but when everybody knows each other in the team and there is good communication about what people are doing this tends to not be the case. It also allows the people who are free to get to the MR rather than possibly interrupting people in the middle of something else.\n* When I use the terms merge request (MR) and pull request (PR) I so do interchangeably.\nDelete merged branches Good housekeeping is to remove branches once they have been merged. When using lots of branches the number of merged branches hanging around can get a bit out of hand. Removing them as soon as they have been merged solves this problem.\nKeep a useful history As has been discussed above, keeping commits small and focused is a good idea. Following this principle is a great way of achieving a clean and more importantly, a useful history.\nMRs introduce an opportunity for that guidance to be difficult to follow. For example, if there have been several small commits created in order to build a feature - when that feature comes to be merged, should those commits be retained or should they be squashed into a single commit? Prior to April 1st 2016 the only option on GitHub was merge commits. A merge commit retains the details of the history of the branch unless those commits had been squashed by the branch owner. I used to think having the branch history squashed was a good idea as it would be a clean entry. However, I now think the benefit of having the detailed commit history and how the feature was built is more valuable. That is not to say squashing commits that are not \u0026lsquo;complete\u0026rsquo; should not be done - they should be squashed if they provide no value.\nIn summary, I think having both merge commits and squash merging options available on a GitHub repository is good.\nThe most important thing to keep in mind is that commits should provide value to the code base and by following the guidance here you will be on the way to doing so.\n","permalink":"https://blog.st3v3nhunt.me/posts/how-i-work-with-git/","summary":"\u003cp\u003eI really enjoy introducing people to Git. Pretty much without fail they see how\nmuch better their development work-flow can be and find their work more\nenjoyable as a result. This is especially true if they are coming from a TFS\nbackground. See how I work with\n\u003ca href=\"../how-i-work-with-git-and-tfs/\"\u003eGit and TFS\u003c/a\u003e.\nHowever, with pleasure comes pain. For Git, that pain is in getting over the\nlearning curve. The size and steepness of the curve varies depending on\nprevious experience of\n\u003ca href=\"https://en.wikipedia.org/wiki/List_of_version_control_software\"\u003eVersion Control Systems\u003c/a\u003e.\nIn my experience, TFS does not provide a good base knowledge to work from as it\ntries to hide as much as possible from the user. Clearly Git is doing a lot for\nyou too but the way it works is more exposed. For example, even the most basic\nof things like adding files into the index can seem strange to a TFS user.\u003c/p\u003e","title":"ü§î How I work with Git"},{"content":"Ever since the recent lengthy and complete outage of Gliffy, I have been thinking about how I could (should that be a should?) protect myself when working with SaaS. Everything is sorted now and Gliffy have a post explaining a bit more detail on the outage. One of the takeaways from that being disaster recovery plans should probably be practiced more often.\nI tend to be quite trusting of SaaS and often come to the conclusion the people running the services are better at running them and having disaster recovery plans in place (and practiced) than I would be if I tried to implement some form of backup. They also tend to be more invested in the continued success of the service than I do as I\u0026rsquo;m guessing it pays some of their bills. However, I increasingly think I‚Äôm probably on the wrong side of the fence on this and I should be erring on the side of caution a bit more.\nOf course, how trusting I am willing to be depends on what and how I‚Äôm using the SaaS. If it is just a ‚Äòplaything‚Äô I can probably leave it as is and not worry if the service goes down and never comes back. For example, I had a handful of diagrams saved in Gliffy that I did not need access to during the downtime and they mean so little to me that if I had never got them back I wouldn\u0026rsquo;t be particularly bothered. However, if I had needed them for a talk or to look at or to hand in I\u0026rsquo;d have been pretty miffed.\nThis leads me to wonder if I should designate different levels of importance to different SaaS. However, that would not be good if I at some time decided to save something more important in a service that I had previously seen as less important. And I don\u0026rsquo;t want to have to remember each time I use a service what level of importance I might have placed on it. So from the POV of making it easy for myself I think having a single level of importance for everything would make things easier. Then it is just a case of having to answer a single question \u0026ldquo;Do I have a backup in place for this service\u0026rdquo;?\nWhich leads onto looking for the answer as to what a good backup solution might be\u0026hellip;\n","permalink":"https://blog.st3v3nhunt.me/posts/gliffy-outage-lessons/","summary":"\u003cp\u003eEver since the recent lengthy and complete\n\u003ca href=\"http://support.gliffy.com/entries/98911057-RESOLVED-Gliffy-Online-System-Outage-\"\u003eoutage of Gliffy\u003c/a\u003e,\nI have been thinking about how I could (should that be a should?) protect\nmyself when working with SaaS. Everything is sorted now and Gliffy have a\n\u003ca href=\"https://www.gliffy.com/blog/2016/03/25/turtle-and-the-hare/\"\u003epost\u003c/a\u003e explaining\na bit more detail on the outage. One of the takeaways from that being disaster\nrecovery plans should probably be practiced more often.\u003c/p\u003e","title":"üìâ What lessons should one learn from the Gliffy outage?"},{"content":"I was recently asked to provide a link to an article that had resonated with me in the recent past. The first thing I did was provide a link to my twitter account to that person. Thinking (hoping) the question would be answered. The second thing I did was to go through some of my time line to see what articles I had tweeted about. One of the things I found was I didn\u0026rsquo;t tweet all that often about articles I find align well to how I think. There are some but there is a lot of stuff I read and do nothing with. There are even fewer tweets about things I don\u0026rsquo;t feel align well with me which is probably a good thing!\nThis got me thinking it might be a good idea to collate a list of articles I find interesting / have resonance with me. This is the first in what I hope will be a series. I will aim for once every two weeks and see how well I can stick to the schedule. As this is the first week the articles go back quite some time which is most likely going to change in the future, as in probably soured from the previous two weeks. I have a feeling the format will change too, as will how closely related the articles are to one another.\nArticle: How Many Decimals of Pi Do We Really Need?\nCommentary: For the most precise calculations NASA does it uses 15 decimal places of Pi, yet it has been calculated to over 12 trillion! The post demonstrates how it is not necessarily critical to know everything, especially about something that is not possible to know everything about before it is OK to use it. I see this an allegory for how organisations spend so much effort on trying to know more about things that have an inherently unending point of what can be known before actually starting the doing.\nArticle: Why \u0026lsquo;Agile\u0026rsquo; and especially Scrum are terrible\nCommentary: A well written piece exploring the \u0026lsquo;bad\u0026rsquo; side of what a lot of companies believe to be the panacea to making everything better, faster, more profitable or should that be Harder Better Faster üòú.\nArticle: The Wrong Conversation\nCommentary: A great post demonstrating how easy it is to hide useful performance information within a target. And how that can lead to focusing on the wrong things.\nArticle: Manage value not cost \u0026amp; If you manage costs\u0026hellip;\nCommentary: A short visual and written companion piece of how managing value rather than costs has counter intuitive (for many of us at least) results i.e. the costs go down!\n","permalink":"https://blog.st3v3nhunt.me/posts/interesting-articles/","summary":"\u003cp\u003eI was recently asked to provide a link to an article that had resonated with me\nin the recent past. The first thing I did was provide a link to my\n\u003ca href=\"https://twitter.com/st3v3nhunt\"\u003etwitter account\u003c/a\u003e to that person. Thinking\n(hoping) the question would be answered.\nThe second thing I did was to go through some of my time line to see what\narticles I had tweeted about. One of the things I found was I didn\u0026rsquo;t tweet all\nthat often about articles I find align well to how I think. There are some but\nthere is a lot of stuff I read and do nothing with. There are even fewer tweets\nabout things I don\u0026rsquo;t feel align well with me which is probably a good thing!\u003c/p\u003e","title":"üë∫ Interesting articles (week one)"},{"content":"This post has been one I\u0026rsquo;ve been meaning to write for months and months. Finally, I\u0026rsquo;ve got round to it! This does mean specifics are going to be few and far between\u0026hellip;\nTL;DR: Use git-tfs as a bridge between the awful TFS and the awesome Git.\nI work in an organisation where the majority of the code is stored in TFS. I came from a background where I used Subversion. As soon as I started to use TFS I hated it. I can\u0026rsquo;t remember the full list of reasons why I had such a strong dislike to it (other than because it was different üòâ) beyond the fact:\nUsing it outside of Visual Studio was (probably still is) a very bad idea. The connected mode is a real nuisance when working with other people in the same area of the code. The way files were all read only. Locked workspaces. These points probably all boil down to the always connected mode and that might have changed since I stopped using it\u0026hellip;I gave up using TFS directly almost immediately after having first been exposed to it.\nMy first foray into hiding the awfulness of TFS was to use a Git to TFS bridge called Git-TF. I used this for several months before I found there were some short-comings with it, that, at the time I was unable to work around. I forget exactly what they were but I have a feeling the ability to deal with files names over 256 characters in length might have had something to do with it.\nOn the lookout for an alternative I came across git-tfs which I still use to this day. I have found git-tfs to be an invaluable companion ever-since, even if the file length problem still exists. Fortunately it is easily worked around with the --workspace= option.\nAnother niggle, which is nothing to do with how git-tfs works but is a result of the structure of the code in the TFS repo is how I need to clone multiple directories within a number of repos in order to have some semblance of separation in how the code lives on my development machine. Add to that the way branching is done and quick-clone has become one of my favourite commands!\nThe final piece to the way I work with TFS puzzle is using TFS Sidekicks. I use this to do things I could do with the git-tfs but am too lazy to do such as a quick visual check on the history of the repo or, and this is its\u0026rsquo; main use, to view other people\u0026rsquo;s shelvesets.\n","permalink":"https://blog.st3v3nhunt.me/posts/how-i-work-with-git-and-tfs/","summary":"\u003cp\u003eThis post has been one I\u0026rsquo;ve been meaning to write for months and months.\nFinally, I\u0026rsquo;ve got round to it! This does mean specifics are going to be few and\nfar between\u0026hellip;\u003c/p\u003e","title":"üòï How I work with Git and TFS"},{"content":"Pipelines in Heroku are pretty awesome. This is how I went about setting up a Node.js app to have automated deployments for all pull requests, an automated deployment to the staging environment and a manual stage for promoting to the production environment. Pretty much a bread and butter pipeline. (This is an image heavy post)\nPre-requisites Existing Node.js app hosted in GitHub (it doesn\u0026rsquo;t need to be GitHub. Dropbox and local git repos are supported by Heroku but pipelines have special features for GitHub repos). Feel free to fork the node-js-getting-started repo I used. An Heroku account (obvs üòâ). Setting up the pipeline An app for staging and review apps for pull requests I started by creating a new app in Heroku. I named it name-not-in-use-staging, note the end of the name i.e. -staging. I did this to make it easier to recognise which app is which when I add them to the pipeline: On the same screen I selected Show more options... so I could add the app to a new pipeline. I named the pipeline pipeline-test and I assigned the app to the staging slot: I then selected Create App which goes to this screen: Note the deployment method section. I followed the instructions and connected the app to my GitHub repo - st3v3nhunt/node-js-getting-started. I also opted for automatic deployments from the master branch, as seen below: A little further down the screen there is the Review apps section. As this app is part of a pipeline, review apps are managed by the pipeline.\nFollowing the link to manage review apps shows us the visualisation of the pipeline for the first time: It\u0026rsquo;s worth noting if your repository doesn\u0026rsquo;t include an app.json file you will be prompted to create one for the configuration of review apps. The default options will be sufficient for getting the pipeline up and running. {: .notice}\nSelecting to Enable Review Apps pops up this screen: Where I opt for creating a review app for every pull request automatically.\nWhen review apps are configured the pipeline will look like this: Note there is nothing in the production slot. Next I will go through what to do in order to make it possible to have your staging app be promoted to production.\nAdd an app for production In order to have an app in production I need to create another app. In the screen below, note how the name is appended with -prod. As with the staging app, this is just a convention I use to help identify which app is which. In this step I have assigned the app to the production slot within the pipeline.\nThe pipeline is now complete. It will look something like this: Pipeline in action Over the next few screens I\u0026rsquo;ll show what happens to the pipeline as a pull request is created and flows through the application from being merged into master then promoted to production.\nPull requests are automatically deployed as part of the review apps functionality. The review app gives people involved in the pull request the opportunity to test the changes in an environment that is not where is was developed. No more \u0026lsquo;works on my machine\u0026rsquo; cries! Not only is this good for providing an isolated test environment it also gets around problems that geographically dispersed people can find themselves in. I\u0026rsquo;ve had numerous occasions where the person I wanted to run through something with was not able to access my development machine. No more will that be a problem!\nOnce a pull request has been merged into master it will be automatically deployed into the staging environment: As seen in the screen above, there is now a button to promote to production.\nClicking on the \u0026lsquo;Promote to production\u0026hellip;\u0026rsquo; button pops up the promote screen: Simply selecting \u0026lsquo;Promote\u0026rsquo; and when the changes have been promoted to production this is what the pipeline will look like: Additional pull requests will go through the same cycle.\nFurther reading Clearly this is a simple setup. Hopefully it has shown just how easy it is to get a pipeline setup in Heroku.\nThere is a host of guides available on Heroku, the pipeline guide is good place to start if you are interested in finding out more.\n","permalink":"https://blog.st3v3nhunt.me/posts/how-to-setup-a-simple-heroku-pipeline/","summary":"\u003cp\u003e\u003ca href=\"https://devcenter.heroku.com/articles/pipelines\"\u003ePipelines in Heroku\u003c/a\u003e are\npretty awesome. This is how I went about setting up a Node.js app to have\nautomated deployments for all pull requests, an automated deployment to the\nstaging environment and a manual stage for promoting to the production\nenvironment. Pretty much a bread and butter pipeline.\n\u003cem\u003e(This is an image heavy post)\u003c/em\u003e\u003c/p\u003e","title":"„Ä∞Ô∏è How to setup a simple Heroku pipeline"},{"content":"This is a post I have been sat on for some time. In fact the date of the day in question was 01/02/2016 so some details are going to be a little inaccurate but hopefully the essence is maintained.\nOnce upon a time I participated in a hack day where not a single line of code was written. \u0026ldquo;Hang on\u0026rdquo;, I hear you cry. How can you have a hack day without doing any hacking? Indeed, this is exactly what I was wondering. Looking around the Internet for definitions and details of hack days, the vast majority of them talk about technology and software as a core part of the day. However, there are a number of information sources where hack days are discussed in the context of no software hacking being involved. I think in these instances it is better to think of the day as a product ideation day which is exactly what it turned out to be. Not quite as snappy a title as \u0026lsquo;hack day\u0026rsquo; though üòâ!\nWith that cleared up, what was the day like?\nWhat did we do during the day? Met new people Selected a user need (from a pre-defined list) Fleshed out the user need (with a micro-persona and acceptance criteria) Expanded on the solution/need Refined the solution Created a (paper based) prototype Carried out user research (with members of the other groups) Iterated the solution based on feedback Replayed it all to the group (providing everybody a chance to see each others prototype and gain some insight into each group\u0026rsquo;s thinking) Outputs from the day Beyond the opportunity to meet and work with (potentially) an entirely new group of people, thinking from a number of different POVs is a really refreshing exercise. Empathising is an underused skill by most people and this day really requires people to empathise. A lot. From thinking about how a person/people within a set scenario might actually be feeling and how that translates into generating their \u0026rsquo;needs\u0026rsquo; to thinking about what the future might look like for them individually and society as a whole. From understanding the new people you are working with in double quick time in order to combine to form a productive group to thinking about how other groups saw the challenge and solved it. The process is much more important than the outcome in days like this.\nChanges I would make for future days Name the day more clearly (it helps to set expectations early) Ensure the constraints being worked within are well understood by participants e.g. it needs to be something possible to build tomorrow vs something that might be possible in 5 years time. This helps prototypes to be of a similar \u0026lsquo;feel\u0026rsquo; rather than having some being current period solutions and others being far-future Sci-Fi dreams! Unless that is what you want in which case go for it. Summary I enjoyed the day and will look to participate in future days. I highly recommend others getting involved given the chance. And if not given the chance it is reasonably easy to set something up yourself.\n","permalink":"https://blog.st3v3nhunt.me/posts/hack-day/","summary":"\u003cp\u003eThis is a post I have been sat on for some time. In fact the date of the day in\nquestion was 01/02/2016 so some details are going to be a little inaccurate but\nhopefully the essence is maintained.\u003c/p\u003e","title":"‚öíÔ∏è  Hack day lite"},{"content":"After having added a new remote to my local git repository via git remote add other-remote git@github.com:user/other-repo I wanted to checkout one of the branches from that remote.\nI initially tried git checkout -b local/branch-name other-remote/feature/branch and got an error. The problem with trying to do this is there is no information locally about what the newly added remote has. In order to rectify this I ran git fetch other-remote. The information about that remote gets downloaded and I am now able to checkout branches from my newly added remote.\nReferences: git-fetch\n","permalink":"https://blog.st3v3nhunt.me/posts/fetch-remote-git-branches/","summary":"\u003cp\u003eAfter having added a new remote to my local git repository via \u003ccode\u003egit remote add other-remote git@github.com:user/other-repo\u003c/code\u003e I wanted to checkout one of the\nbranches from that remote.\u003c/p\u003e","title":"üéã Fetch remote Git branches"},{"content":"I have written a bit about how to make your commit messages more insightful.\nIf the idiom a picture is worth a thousand words is true, perhaps using emoji to add an extra level of information to your commit messages is a good idea? Even if it is not, I would recommend adding emoji to commit messages.\nEmoji add some \u0026#x2728; to your commits. Looking back over the history has never been so colourful. A lot of source control web interfaces support emoji so having an emoji on every commit can brighten up the code home page of your project, possibly making it more attractive to potential contributors. And everybody likes the idea of more people working on their project, right?\nHow do I choose an appropriate emoji? Given there are so many emoji, how do you know which to use?\nWhilst there are no \u0026lsquo;official\u0026rsquo; rules about when and how to use emoji I would like to propose the following guidance (perhaps this could be the first officially recognised rule? :wink:):\nOnly \u0026#x31;\u0026#xfe0f;\u0026#x20e3; \u0026#x1f4a5; allowed per repository. It can only be the very first commit, if not used for the first commit it is not permitted within the repository. Should a \u0026#x1f4a5; be used within any subsequent message the entire commit shall be rejected during the pull/merge request process. It was once said to me that the choice of the emoji should be based on how it made one feel, the emotional state of the commit made real via emoji. I find this to be a good place to start but occasionally tricky. When this approach fails I fall back to looking for an emoji to represent what has changed or ideally, why the change was made.\nAn example would be when finishing up a feature, knowing that commit will be pushed to the production environment. The :shipit: emoji is apt or a bug fix the generic :bug: can suffice.\nWhat do you think to adding emoji to your commits? Is this a practice you follow and do you have any guidance for the uninitiated?\n","permalink":"https://blog.st3v3nhunt.me/posts/how-much-is-an-emoji-worth/","summary":"\u003cp\u003eI have written a bit about how to make your commit messages more\n\u003ca href=\"are-your-commit-messages-insightful/\"\u003einsightful\u003c/a\u003e.\u003c/p\u003e","title":"üíµ How much is an emoji worth"},{"content":"A good commit message is a hard thing to do.\nWhy do I say this? From my experience most of the commit messages are a repetition of information contained in the commit itself. An example would be something along the lines of where a change to the colour of a button was required. Perhaps the button was changed from red to blue, the reason for the change was due to user research showing blue was more enticing to the customer, resulting in more clicks of the button (a good thing).\nFor this type of change I would not be surprised to see a commit message along the lines of button changed from red to blue. Whilst this is not untrue it doesn\u0026rsquo;t give your future self much to go on as to the reason for the change. It also repeats information that can be gained by looking at the commit.\nIn my opinion, a more useful message would include the reason why the change was made. Something like blue button attracts more clicks. If you or I were to look back at the change in 6 months it becomes very clear why the button colour was changed. This could prevent an arbitrary change of colour, possibly reducing the number of clicks, from being made. I\u0026rsquo;m not sure the first message would have provided that insight.\nHow do you make your commit messages provide value in the future? Is there a recipe that can be followed?\n","permalink":"https://blog.st3v3nhunt.me/posts/are-your-commit-messages-insightful/","summary":"\u003cp\u003eA good commit message is a hard thing to do.\u003c/p\u003e","title":"üîé Are your commit messages insightful?"},{"content":"There are so many resources available to help you get going with AngularJS it can be difficult to know where to start.\nWhy not start at the beginning, the AngularJS website. There are plenty of links to useful things like the free Code School course, the developer docs, the blog, social media accounts and much more.\nWhat did I do? These are the first steps I took on my AngularJS journey:\nCompleted the free course \u0026lsquo;Shaping up with AngularJS\u0026rsquo; by Code School Completed the free course \u0026lsquo;Learn AngularJS\u0026rsquo; by Codecademy Signed up to ng-newsletter Read some of the posts at the blog Shaping up with AngularJS This is a good first step. Video tuition followed by practical exercises. The exercises hold you by the hand but this is a good thing. Overall, a well presented intro to ease you into AngularJS.\nLearn AngularJS More involved than the Code School course. No videos, just written instructions. Less hand holding but fewer nuggets of related useful information. Doing this course after the Code School felt like the correct order.\nThe blog The first post I read was about the Angular 2 Survey Results. I didn\u0026rsquo;t even know there was an Angular 2, so immediately I was learning. The post has introduced a raft of other things I will go and investigate; build tools, server rendering, data libraries, etc. No doubt, it will continue to be a source of inspiration and useful information.\n","permalink":"https://blog.st3v3nhunt.me/posts/getting-started-with-angularjs/","summary":"\u003cp\u003eThere are so many resources available to help you get going with\n\u003ca href=\"https://angularjs.org/\"\u003eAngularJS\u003c/a\u003e it can be difficult to know where to start.\u003c/p\u003e","title":"üìè Getting started with AngularJS"},{"content":"The majority of my coding time is spent in the back-end, so to speak. Mostly within a Microsoft environment. When an opportunity arises to discard the shackles of Visual Studio I figuratively leap at the chance.\nLucky for me, just such an opportunity presented itself in the not so distant past. It wasn\u0026rsquo;t going to be all sweetness Sublime Text and roses Chrome DevTools but enough to whet the appetite for something less, well, less\u0026hellip;restrictive (mostly in the sense of the systems that have been built as opposed to the tooling).\nBest practices I haven\u0026rsquo;t done too much front-end development over the past couple of years so I delved into the depths of my memory (unpleasant) and pulled out what I thought were good practices for writing JavaScript. Things like:\nThe revealing module pattern Google\u0026rsquo;s Code Style JSHint Immediately-invoked function expression As it happens, these are still good things to be doing. Check out JavaScript The Right Way for more comprehensive information.\nDOM manipulation When it came to manipulating the DOM I instinctively turned to what I had used in the past - JQuery. Chosen as much because I had experience with JQuery as it was because I didn\u0026rsquo;t have a choice.\nThe work was completed and I was quite happy with the relatively minimal DOM manipulation I had managed (choosing to show/hide pre-built elements within the page, as opposed to building lots of stuff within JavaScript). However, I was left feeling disappointed the code was not better.\nI wanted more. I knew of some front-end JavaScript frameworks (amongst the many hundreds of others):\nAngularJS Backbone.js Ember.js knockout.js I was confident I needed to embrace one of them. This would make both myself and the code I produce better than the current situation.\nChoices, choices, choices But which one to try? Sure, I could try them all (and I probably should), however, I wanted to know which to try first.\nColleagues consulted, Google searched, GitHub\u0026rsquo;s front-end JavaScript frameworks showcase reviewed. There was a clear leader - AngularJS.\nDecision made - AngularJS it is Why? Well, it seems to be The Framework¬© all the Cool Kids¬© are using. It\u0026rsquo;s backed by Google. Using some (admittedly fairly arbitrary) comparisons between the frameworks (all correct at the time of writing) it comes out as a clear leader.\nGitHub Stars: 42,311 for AngularJS 22,886 for Backbone.js 14,693 for Ember.js 6,705 for knockout.js Questions tagged in Stackoverflow: AngularJS - 119,314 Backbone.js - 18,622 Ember.js - 16,546 Knockout.js - 14,490 Google searches over time ","permalink":"https://blog.st3v3nhunt.me/posts/choosing-a-front-end-javascript-framework/","summary":"\u003cp\u003eThe majority of my coding time is spent in the back-end, so to speak. Mostly\nwithin a Microsoft environment. When an opportunity arises to discard the\nshackles of Visual Studio I figuratively leap at the chance.\u003c/p\u003e","title":"üìê Choosing a frontend JavaScript framework"},{"content":"Using git branch -r to check the list of branches on the remote can often lead to a long list of branches making it difficult to tell which are active e.g.\nThe list is easily trimmed to only those that are active. Git calls this pruning and is achieved via git remote prune origin. There is a test mode activated by the flag --dry-run e.g. git remote prune origin --dry-run, resulting in output like this:\nWhen run without the flag (git remote prune origin) the branches are pruned as so:\n","permalink":"https://blog.st3v3nhunt.me/posts/prune-remote-git-branches/","summary":"\u003cp\u003eUsing \u003ccode\u003egit branch -r\u003c/code\u003e to check the list of branches on the remote can often\nlead to a long list of branches making it difficult to tell which are active\ne.g.\u003c/p\u003e","title":"‚úÇÔ∏è  Prune remote git branches"},{"content":"A collection of Powershell snippets I have found useful.\nSet the title of the window: (Get-Host).UI.RawUI.WindowTitle = \u0026#34;New Window Title\u0026#34; Reload the path for the session (source): $env:Path = [System.Environment]::GetEnvironmentVariable(\u0026#34;Path\u0026#34;,\u0026#34;Machine\u0026#34;) Delete all files/folders without confirmation messages: rm -r -force .\\path\\to\\dir List all environment variables with Get-ChildItem and its\u0026rsquo; aliases: Get-ChildItem Env: gci Env: dir Env: ls Env: Get specific environment variable using the Path as an example. The first option returns just the value whereas the second returns it in a table format: $Env:Path gci Env:Path Set specific environment variable: $env:Path = \u0026#39;new value goes here\u0026#39; $Env:Path = \u0026#39;new value goes here\u0026#39; ","permalink":"https://blog.st3v3nhunt.me/posts/powershell-snippets/","summary":"\u003cp\u003eA collection of Powershell snippets I have found useful.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSet the title of the window:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-powershell\" data-lang=\"powershell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e(Get-Host).UI.RawUI.WindowTitle = \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;New Window Title\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pr","title":"üîå Powershell snippets"},{"content":"The team I work in uses Trello to visualise our work. Some of the team have been using similar tools in order to facilitate work visualisation for several years. Other members are much newer to the concept. Recently the team had a refresher session to bring everybody up to a similar level of understanding.\nWithout going into the details of how and why we visualise our work (future post), I wanted to share a few of the hints / tips / best practice we use:\nOver communicate. It doesn\u0026rsquo;t take much effort to type a handful of words and can save others time. Direct messages with @mentions rather than relying on people being subscribed to a card. Subscribe to cards rather than assigning yourself. In order to see who is working on a card, only have those people actively working on it assigned. Assign yourself to a single card. Unless you can truly multitask (hint, you can\u0026rsquo;t!). Use cover images on a single card in a column, located at the top, communicating the column\u0026rsquo;s policies. Everybody should have a consistent avatar across all systems the team uses e.g. Skype, Twitter, Slack, etc. Do not use corporate email to interact with Trello. Corporate email has additional spiel added which clutters up the card. Turn on card aging to help identify abandoned and blocked cards. Use filters to help you find what you need. Save yourself time by learning the keyboard shortcuts. Use labels to categorise cards e.g. \u0026lsquo;User story\u0026rsquo;, \u0026lsquo;Technical\u0026rsquo;, etc. Sprinkle emojis through out. They add some colour and it just so happens to be the UK\u0026rsquo;s fastest growing language. Whilst most of the points above have been in place for a while, some are new to the team. We will be testing them out and only adopting them if we deem them to have been successful.\nThe team has a bunch of other ideas to test out which I shall share in due course.\nIf you haven\u0026rsquo;t already, why not give Trello a go yourself?\n","permalink":"https://blog.st3v3nhunt.me/posts/tips-for-using-trello/","summary":"\u003cp\u003eThe team I work in uses \u003ca href=\"https://trello.com\"\u003eTrello\u003c/a\u003e to visualise our work.\nSome of the team have been using similar tools in order to facilitate work\nvisualisation for several years. Other members are much newer to the concept.\nRecently the team had a refresher session to bring everybody up to a similar\nlevel of understanding.\u003c/p\u003e","title":"üìá Tips for using Trello"},{"content":"How do you generate attributes for html elements that have a dash (-) in them through a C# view template engine while using object initialiser syntax?\nUse an underscore (_) instead.\nAn example. In a Razor view using the Html.ActionLink(...) helper to create an anchor element with an attribute called data-values the code would be:\n@Html.ActionLink( \u0026#34;link text\u0026#34;, \u0026#34;actionName\u0026#34;, new { Model.Id }, new { data_values = \u0026#34;some-value-for-the-data\u0026#34; }) Sourced from Stackoverflow\n","permalink":"https://blog.st3v3nhunt.me/posts/dashed-html-attribute-in-c-sharp-view-engine/","summary":"\u003cp\u003eHow do you generate attributes for html elements that have a dash (\u003ccode\u003e-\u003c/code\u003e) in them\nthrough a C# view template engine while using object initialiser syntax?\u003c/p\u003e","title":"„Ä∞Ô∏è Dashed html attribute in C# view engine"},{"content":"Using MongoDB to build an API providing some search and filtering functionality. One of the searches was using the $near query. As part of the call the total number of documents matching the query also needed to be returned.\nThere was a query builder responsible for building the queries. Rather than distinguish between a query for a count and a query to return the documents the same query was built and used for both operations. During performance testing there was a noticeable delay in returning data for calls including $near queries.\nA bit of investigation and a little more clearer thinking and the $near query element being included within the count() query makes no sense whatsoever. Plus, it is no good for performance. No reduction in the number of results occurs through $near so why would it be included within count() queries? Well, it should not be.\nRemoving $near from count() has improved the performance markedly. In this specific case it reduced the time taken to run the query by at least an order of magnitude.\nA silly oversight that should have been picked up during a code review and one that could have proved a little more costly than it did, had it not been found through performance testing. Interesting nevertheless.\n","permalink":"https://blog.st3v3nhunt.me/posts/do-not-include-near-in-count-queries/","summary":"\u003cp\u003eUsing MongoDB to build an API providing some search and filtering\nfunctionality. One of the searches was using the \u003ccode\u003e$near\u003c/code\u003e query. As part of the\ncall the total number of documents matching the query also needed to be\nreturned.\u003c/p\u003e","title":"üî¢ Do not use include near in count queries"},{"content":"What is a CFD? A CFD is a graph showing the state of the work over a period of time based on data acquired from Kanban.\nMore in-depth explanations available at Wikipedia, Google and David J Anderson.\nGraphs and colours‚Ä¶I want one To create a CFD is pretty straight forward. The number of stories in each state represented on the board need to be recorded on a regular basis. Daily is typical. With the data a chart can be generated, plotting the accumulated data against time.\nColours are great but why do I need one? There is a host of reasons why using CFDs are a good idea. Personally, I like the fact that from a relatively small amount of easily obtainable data a huge amount of information can be discovered. The information can provide some very revealing insights into the flow of the work.\nInsights sound interesting, like what‚Ä¶ It is probably best to use a real world example. The commentary and insights below were gained after studying this CFD.\nThe CFD is from a project where the time and cost were fixed with the scope being variable. Effectively a time box where as much as possible was to be delivered.\nBacklog (ideas hangout) Although there are stories added throughout the duration of the project there are a couple of big bangs (19/08 \u0026amp; 31/08). Ideally all stories should be added on a regular, free flowing basis. Approximately 40% of the stories created were not done - how much time was spent on creating the stories that were effectively wasted (not much)? Refinement (ideas thought about) There were always some stories being \u0026lsquo;refined\u0026rsquo;. One might assume that is a good thing as stories were always being thought about and made ready for development. UX \u0026amp; Design (interface) The state was only used within the first couple of weeks. This might have been due to that being the time when the front end was being designed and refined before it was developed. Ready to go (blast off) A large number of stories were ready to go in week 2, for the full week. Not one story moved from this state during the week. Not a good thing, the board will have looked pretty stagnant at this time. A couple of stories ended the time box as being refined - this means time and effort was spent on doing the work to get them into this state and that was wasted. In progress (rolling) Week 3 saw many stories in progress, perhaps too many. How many people were in the team and how many of the stories were actually being worked on? Were there blockers stopping the progress of the stories? Ready to verify (awaiting testing) Stories were not sat in the queue for very long. Actively being tested as soon as available. Bit of queue building up towards the end of the time box, was this caused by the knowledge the time was closing? Verfying (is it ok) Some stories were stuck in verifying for a long time. Possibly there was no environment available to complete the verification? Verified (done the thing right) Some of the stories are in verified for a small period of time, suggesting they were accepted almost immediately. The last week or so saw a few stories stack up in verified. This was due to environment problems. Once that was sorted the stories quickly moved into accepted. Accepted (done the right thing) It was 2 weeks before anymore than 2 stories were accepted. It looks like the stories were accepted in chunks rather than when they were ready to be accepted. Is this due to the person accepting them not being available or engaged or some other reason? Waste (‚Ä¶) The stories added around 19/08 - were they wasted 27/08? If so, why were they created and perhaps this should not happen in the future? ","permalink":"https://blog.st3v3nhunt.me/posts/cumulative-flow-diagrams/","summary":"\u003ch2 id=\"what-is-a-cfd\"\u003eWhat is a CFD?\u003c/h2\u003e\n\u003cp\u003eA CFD is a graph showing the state of the work over a period of time based on\ndata acquired from\n\u003ca href=\"http://st3v3nhunt.tumblr.com/post/61110223853/what-work-has-work-where-working-work-wrangles-waiting\"\u003eKanban\u003c/a\u003e.\u003c/p\u003e","title":"üìà Cumulative flow diagrams"},{"content":"This is a little bit about how work at where I work works.\nVisualisation of work Kanban. Is the system used to visual the work. It includes who is involved, what they are involved in, what state work is in and whether there are things getting in the way. The currency of the work is a story. The more stories completed, the richer everybody is. Flow is everything.\nThe currency of work The vessel of all work is a story. Stories have a size, small (S), medium (M) or large (L). The size of a story is dictated by the complexity of the work involved. This is often based on the number of systems that will need to be changed to complete the work.\nWhat does a story look like? There is no set definition of what a story is (perhaps there never will be although it would probably be a good idea). The level at which a story is created varies. Some of the experiences differing levels of story have provided are:\nOne project had stories focused on the technical activities required to get the work done. They were very much like tasks. Stories at this level moved through the board quickly, this built momentum. It worked well for technical people. It did not work well for non-technical people. The board was flooded with stories that made little sense to business people. However good it might have been for the technical team, not having the business understand the state of the project is not a good position to be in. Especially when a goal of the project is to increase the level of participation of the business in the development of products.\nAnother project had stories written at a much higher level, one where they would provide some real value to the Product Owner (PO). There were far fewer stories and each story was much clearer about what was being delivered. This approach worked much better from the point of view of looking at the board and understanding the state of the project. Business people were much more bought in. This is a very good thing. However, it was not all roses and sunshine. The major loss was the flow of stories through the board. The board stagnated. How to improve on this is an on-going concern. Probable solutions revolve around another board, a \u0026rsquo;technical' board, however, this has its\u0026rsquo; own challenges.\nFree flowing One of the goals of Kanban is to increase the rate at which stories flow through the board. This is referred to in two ways - cycle time and lead time.\nLead time This is how long it takes a story to get across the entirety of the board. From initial state to end state. It is useful to see how long an idea takes to go from inception to realisation. Often used by the business.\nCycle time This is how long is takes a story to get to done i.e. from the time it leaves the backlog until it reaches the end state. Provides information on the time it takes to do the work. Used by the team doing the work.\nWhy should anybody care about lead and cycle time? There are many reasons to care about lead and cycle times. My number one reason is, the smaller the time it takes to get something to the end state (which should really be the production environment) the quicker the product can be iterated.\nThe quicker the product can be iterated, the quicker the wants of the users of the product can be met, the happier those users can be made, the more users there will be. Ultimately ending in the beating of competitors whilst providing the best product possible, built based on the needs of the users. Win, win.\n","permalink":"https://blog.st3v3nhunt.me/posts/how-do-we-work/","summary":"\u003cp\u003eThis is a little bit about how work at where I work works.\u003c/p\u003e","title":"üöß How do we work"}]